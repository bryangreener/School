\documentclass{report}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts}
\usepackage{tabto}
\usepackage[yyyymmdd]{datetime}
\renewcommand{\dateseparator}{--}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}

% For clickable TOC
\usepackage{hyperref}
\hypersetup{
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black
}

% For definitions
%\newtheorem{defn}{Definition}[section]
%\newtheorem{thrm}{Theorem}[section]
%\newtheorem{ex}[Example}[section]
\newtheorem*{ex}{Example}
\newtheorem*{defn}{Definition}
\newtheorem*{thrm}{Theorem}
\newtheorem*{lemma}{Lemma}
\newtheorem*{result}{Result}


% For circled text
\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=0.8pt] (char) {#1};}}

% For equation system alignment
\usepackage{systeme,mathtools}
% Usage:
%	\[
%	\sysdelim.\}\systeme{
%	3z +y = 10,
%	x + y +  z = 6,
%	3y - z = 13}

\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
%If you want to title your bold things something different just make another thing exactly like this but replace "problem" with the name of the thing you want, like theorem or lemma or whatever
 
%used for matrix vertical line
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother 
 
% Change chapter numbering
\newcommand{\mychapter}[2]{
	\setcounter{chapter}{#1}
	\setcounter{section}{0}
	\chapter*{#2}
	\addcontentsline{toc}{chapter}{#2}
}




\begin{document}
 
\tableofcontents{}
\mychapter{1}{2018-01-11}
\section{Gauss-Jordan Elimination}
\begin{align*}
IF \quad M=
\begin{bmatrix}[cccc|c]
1 & 2 & -1 & 2 & 1\\
2 & 4 & 1 & -2 & 3\\
3 & 6 & 2 & -6 & 5\\
\end{bmatrix}
\begin{bmatrix}
R_1\\ R_2\\ R_3\\
\end{bmatrix}\\
%========================================
\begin{bmatrix}
R_1\\
R_2 - 2R_1\\
R_3-3R_1\\
\end{bmatrix}
\begin{bmatrix}[cccc|c]
\circled{1} & 2 & -1 & 2 & 1\\
0 & 0 & \circled{3} & -6 & 1\\
0 & 0 & \circled{5} & -12 & 2\\
\end{bmatrix}
\begin{bmatrix}
R_4\\ R_5\\ R_6\\
\end{bmatrix}\\
%========================================
\begin{bmatrix}
R_4\\
(\frac{1}{3})R_5\\
R_6\\
\end{bmatrix}
\begin{bmatrix}[cccc|c]
1 & 2 & -1 & 2 & 1\\
0 & 0 & 1 & -2 & \frac{1}{3}\\
0 & 0 & 5 & -12 & 2\\
\end{bmatrix}
\begin{bmatrix}
R_7\\ R_8\\ R_9
\end{bmatrix}\\
%========================================
\begin{bmatrix}
R_7\\
R_8\\
R_9 - 5R_8\\
\end{bmatrix}
\begin{bmatrix}[cccc|c]
\circled{1} & 2 & -1 & 2 & 1\\
0 & 0 & \circled{1} & -2 & \frac{1}{3}\\
0 & 0 & 0 & \circled{-2} & \frac{1}{3}\\
\end{bmatrix}
\begin{bmatrix}
R_{10}\\ R_{11}\\ R_{12}\\
\end{bmatrix}\\
%========================================
\begin{bmatrix}
R_{10}\\
R_{11}\\
(-\frac{1}{2})R_{12}\\
\end{bmatrix}
\begin{bmatrix}[cccc|c]
1 & 2 & -1 & 2 & 1\\
0 & 0 & 1 & -2 & \frac{1}{3}\\
0 & 0 & 0 & 1 & -\frac{1}{6}\\
\end{bmatrix}
\begin{bmatrix}
S_1\\ S_2\\ S_3\\
\end{bmatrix}\\
%========================================
\begin{bmatrix}
S_1 - 2S_3\\
S_2 + 2S_3\\
S_3\\
\end{bmatrix}
\begin{bmatrix}[cccc|c]
\circled{1} & 2 & -1 & 0 & \frac{4}{3}\\
0 & 0 & \circled{1} & 0 & 0\\
0 & 0 & 0 & \circled{1} & -\frac{1}{6}\\
\end{bmatrix}
\begin{bmatrix}
S_4\\ S_5\\ S_6\\
\end{bmatrix}\\
%========================================
\begin{bmatrix}
S_4 + S_5\\
S_5\\
S_6\\
\end{bmatrix}
\begin{bmatrix}[cccc|c]
1 & 2 & 0 & 0 & \frac{4}{3}\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 1 & -\frac{1}{6}\\
\end{bmatrix}
\begin{bmatrix}
S_7\\
S_8\\
S_9\\
\end{bmatrix}
\end{align*}
This is now in reduced row-echelon form.\\
\\
\section{Matrix/Vector Algebra}
Vector:
\begin{align*}
\begin{bmatrix}
x_1\\ x_2\\ x_3\\ \vdots \\ x_n
\end{bmatrix}\\
\end{align*}
Matrix Addition:
\begin{align*}
\begin{bmatrix}
2 & 0\\
-1 & 1\\
5 & 4\\
\end{bmatrix}
+
\begin{bmatrix}
1 & 4\\
3 & 1\\
7 & 7\\
\end{bmatrix}
=
\begin{bmatrix}
3 & 4\\
2 & 2\\
12 & 11\\
\end{bmatrix}
\end{align*}
Scalar Multiplication:
\begin{align*}
5 *
\begin{bmatrix}
1 & 4\\
3 & 1\\
7 & 7\\
\end{bmatrix}
=
\begin{bmatrix}
5 & 20\\
15 & 5\\
35 & 35\\
\end{bmatrix}
\end{align*}
Adding vectors can be represented by a graph with vectors on it.\\
Vector addition can be expressed using the tip-to-tail rule.\\
Scalar multiplication can also be expressed by a graph with vectors on it.\\

\begin{defn}
Suppose $V_a=(a_1,a_2,a_3,...,a_n)$ and $V_b = (b_1,b_2,b_3,...,b_n)$ are n-tuples. The dot product of $V_a$ and $V_b$ is $V_a \dot V_b$
\end{defn}

%==========================================

\mychapter{2}{2018-01-12}
\section{Gauss-Jordan Elimination}
\begin{align*}
IF \quad M=
\begin{bmatrix}[cccc|c]
1 & 2 & -1 & 2 & 1\\
2 & 4 & 1 & -2 & 3\\
3 & 6 & 2 & -6 & 5\\
\end{bmatrix}
\begin{bmatrix}
R_1\\ R_2\\ R_3\\
\end{bmatrix}\\
%========================================
\begin{bmatrix}
R_1\\
R_2 - 2R_1\\
R_3-3R_1\\
\end{bmatrix}
\begin{bmatrix}[cccc|c]
\circled{1} & 2 & -1 & 2 & 1\\
0 & 0 & \circled{3} & -6 & 1\\
0 & 0 & \circled{5} & -12 & 2\\
\end{bmatrix}
\begin{bmatrix}
R_4\\ R_5\\ R_6\\
\end{bmatrix}\\
%========================================
\begin{bmatrix}
R_4\\
(\frac{1}{3})R_5\\
R_6\\
\end{bmatrix}
\begin{bmatrix}[cccc|c]
1 & 2 & -1 & 2 & 1\\
0 & 0 & 1 & -2 & \frac{1}{3}\\
0 & 0 & 5 & -12 & 2\\
\end{bmatrix}
\begin{bmatrix}
R_7\\ R_8\\ R_9
\end{bmatrix}\\
%========================================
\begin{bmatrix}
R_7\\
R_8\\
R_9 - 5R_8\\
\end{bmatrix}
\begin{bmatrix}[cccc|c]
\circled{1} & 2 & -1 & 2 & 1\\
0 & 0 & \circled{1} & -2 & \frac{1}{3}\\
0 & 0 & 0 & \circled{-2} & \frac{1}{3}\\
\end{bmatrix}
\begin{bmatrix}
R_{10}\\ R_{11}\\ R_{12}\\
\end{bmatrix}\\
%========================================
\begin{bmatrix}
R_{10}\\
R_{11}\\
(-\frac{1}{2})R_{12}\\
\end{bmatrix}
\begin{bmatrix}[cccc|c]
1 & 2 & -1 & 2 & 1\\
0 & 0 & 1 & -2 & \frac{1}{3}\\
0 & 0 & 0 & 1 & -\frac{1}{6}\\
\end{bmatrix}
\begin{bmatrix}
S_1\\ S_2\\ S_3\\
\end{bmatrix}\\
%========================================
\begin{bmatrix}
S_1 - 2S_3\\
S_2 + 2S_3\\
S_3\\
\end{bmatrix}
\begin{bmatrix}[cccc|c]
\circled{1} & 2 & -1 & 0 & \frac{4}{3}\\
0 & 0 & \circled{1} & 0 & 0\\
0 & 0 & 0 & \circled{1} & -\frac{1}{6}\\
\end{bmatrix}
\begin{bmatrix}
S_4\\ S_5\\ S_6\\
\end{bmatrix}\\
%========================================
\begin{bmatrix}
S_4 + S_5\\
S_5\\
S_6\\
\end{bmatrix}
\begin{bmatrix}[cccc|c]
1 & 2 & 0 & 0 & \frac{4}{3}\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 1 & -\frac{1}{6}\\
\end{bmatrix}
\begin{bmatrix}
S_7\\
S_8\\
S_9\\
\end{bmatrix}
\end{align*}
This is now in reduced row-echelon form.\\
\\
\section{Matrix/Vector Algebra}
Vector:
\begin{align*}
\begin{bmatrix}
x_1\\ x_2\\ x_3\\ \vdots \\ x_n
\end{bmatrix}\\
\end{align*}
Matrix Addition:
\begin{align*}
\begin{bmatrix}
2 & 0\\
-1 & 1\\
5 & 4\\
\end{bmatrix}
+
\begin{bmatrix}
1 & 4\\
3 & 1\\
7 & 7\\
\end{bmatrix}
=
\begin{bmatrix}
3 & 4\\
2 & 2\\
12 & 11\\
\end{bmatrix}
\end{align*}
Scalar Multiplication:
\begin{align*}
5 *
\begin{bmatrix}
1 & 4\\
3 & 1\\
7 & 7\\
\end{bmatrix}
=
\begin{bmatrix}
5 & 20\\
15 & 5\\
35 & 35\\
\end{bmatrix}
\end{align*}
Adding vectors can be represented by a graph with vectors on it.\\
Vector addition can be expressed using the tip-to-tail rule.\\
Scalar multiplication can also be expressed by a graph with vectors on it.\\

\begin{defn}
Suppose $V_a=(a_1,a_2,a_3,...,a_n)$ and $V_b = (b_1,b_2,b_3,...,b_n)$ are n-tuples. The dot product of $V_a$ and $V_b$ is $V_a \dot V_b$
\end{defn}

%===========================================

\mychapter{3}{2018-01-16}
\section{Homework Review}
\begin{enumerate}
\subsection{2.82}
% 2.82
\item [2.82.] Find the values of k such that each of the following systems of unknowns x, y, and z has (i) a unique solution, (ii) no solution, (iii) an infinite number of solutions.
	\begin{enumerate}
	% 2.82.a	
	\item
	\sysdelim{.}{.}\systeme[xyz]{x-2y=1,x-y+kz=-2,ky+4z=6}
	\medskip
		\begin{enumerate}
		% 2.82.a.i
		\item [(i)]
		\begin{align*}
		\begin{bmatrix}[rrr|r]
		1 & -2 & 0 & 1\\
		1 & -1 & k & -2\\
		0 & k & 4 & 6\\
		\end{bmatrix}&
		\begin{bmatrix}[r]
		R_1\\ R_2\\ R_3\\
		\end{bmatrix}\\
		%		
		\begin{bmatrix}[r]
		R_1\\
		R_2 - R_1\\
		R_3\\
		\end{bmatrix}
		\begin{bmatrix}[rrr|r]
		1 & -1 & 0 & 1\\
		0 & 1 & k & -3\\
		0 & k & 4 & 6\\
		\end{bmatrix}&
		\begin{bmatrix}[r]
		R_4\\ R_5\\ R_6\\
		\end{bmatrix}\\
		%
		\begin{bmatrix}[r]	
		R_4\\
		R_5\\
		R_6 - kR_5\\	
		\end{bmatrix}
		\begin{bmatrix}[rrr|r]
		1 & 0 & 0 & -2\\
		0 & 1 & k & -3\\
		0 & 0 & 4-k^2 & 6-3k\\
		\end{bmatrix}&
		\begin{bmatrix}[r]
		R_7\\ R_8\\ R_9
		\end{bmatrix}\\		
		%		
		4-k^2&= 6-3k\\
		k^2-3k+2&=0\\
		(k-2)(k-1)&=0\\
		k=2, \quad k&=1
		\end{align*}
		ANSWER SHOULD BE $\pm 2$.\\
		For a unique solution, $k \neq 2$ and $k \neq 1$.\\
		% 2.82.a.ii
		\item [(ii)] Using the result from 2.82.a.i, if $k=2$ then
		\[ R_9(2): \quad 4-(k)^2 = 0 = 6-3(2) = 0 \]
		Since the result is $0 = 0$, then there are no solutions when $k=2$.\\
		% 2.82.a.iii
		\item [(iii)] 
		\end{enumerate}	
		\medskip
	% 2.82.b
	\item
	\sysdelim{.}{.}\systeme[xyz]{x+y+kz=1,x+ky+z=1,kx+y+z=1}
	\medskip
		\begin{enumerate}
		% 2.82.b.i			
		\item [(i)]
		\begin{align*}
		\begin{bmatrix}[rrr|r]
		1 & 1 & k & 1\\
		1 & k & 1 & 1\\
		k & 1 & 1 & 1\\
		\end{bmatrix}&
		\begin{bmatrix}[r]
		R_1\\ R_2\\ R_3\\
		\end{bmatrix}\\
		%
		\begin{bmatrix}[r]
		R_3\\ R_2\\ R_1\\
		\end{bmatrix}
		\begin{bmatrix}[rrr|r]
		k & 1 & 1 & 1\\
		1 & k & 1 & 1\\
		1 & 1 & k & 1\\
		\end{bmatrix}&
		\begin{bmatrix}[r]
		R_4\\ R_5\\ R_6\\
		\end{bmatrix}\\
		%
		\begin{bmatrix}[r]
		R_4\\		
		R_5 - R_6\\
		R_6 - R_4\\
		\end{bmatrix}
		\begin{bmatrix}[rrr|r]
		k & 1 & 1 & 1\\
		0 & k-1 & 1-k & 0\\
		1-k & 0 & k-1 & 0\\
		\end{bmatrix}&
		\begin{bmatrix}[r]
		R_7\\ R_8\\ R_9\\
		\end{bmatrix}\\
		%
		\begin{bmatrix}[r]
		R_7 + R_9\\
		R_8\\
		R_9\\
		\end{bmatrix}
		\begin{bmatrix}[rrr|r]
		1 & 1 & k & 1\\
		0 & k-1 & 1-k & 0\\
		1-k & 0 & k-1 & 0\\
		\end{bmatrix}&
		\begin{bmatrix}[r]
		R_{10}\\ R_{11}\\ R_{12}\\
		\end{bmatrix}\\
		%
		\begin{bmatrix}[r]
		R_{10}\\
		R_{11}\\
		R_{12} + (k-1)R_{10}\\	
		\end{bmatrix}
		\begin{bmatrix}[rrr|r]
		1 & 1 & k & 1\\
		0 & k-1 & 1-k & 0\\
		0 & k-1 & k^2-1 & 0\\
		\end{bmatrix}&
		\begin{bmatrix}[r]
		R_{13}\\ R_{14}\\ R_{15}\\
		\end{bmatrix}\\
		%
		\begin{bmatrix}[r]
		R_{13}\\
		R_{14}\\
		R_{15} - R_{14}\\
		\end{bmatrix}
		\begin{bmatrix}[rrr|r]
		1 & 1 & k & 1\\
		0 & k-1 & 1-k & 0\\
		0 & 0 & k^2+k-2 & 0\\
		\end{bmatrix}&
		\begin{bmatrix}[r]
		R_{16}\\ R_{17}\\ R_{18}\\
		\end{bmatrix}\\
		%
		(k^2+k-2) &=0\\
		(k+2)(k-1) &=0\\
		k = -2, \quad k &= 1\\
		\end{align*}
		For a distinct solution, $k \neq -2$ and $k \neq 1$.\\
		% 2.82.b.ii
		\item [(ii)]
		From the solution to 2.82.b.i, allowing $k=-2$ gives us
		\[ z((-2)^2+(-2)-2) = z(0) = 0 \]
		Since the result is $0 = 0$, we get no solutions.\\
		% 2.82.b.iii
		\item [(iii)]
		From the solution to 2.82.b.ii, allowing $k=1$ gives us
		\[ z((1)^2 + (1) - 2 = z(0) = 0 \]
		FINISH THIS. ANSWER IS $\neq -2, 1 \quad , \quad -2, \quad 1$ just need to figure out why. I got the answer but I can't seem to reason why.
		\end{enumerate}	
		\medskip
	%2.82.c
	\item
	\sysdelim{.}{.}\systeme[xyz]{x+2y+2z=5,x+ky+3z=7,x+11y+kz=11}
	\medskip
		\begin{enumerate}
		% 2.82.c.i
		\item [(i)]
		\begin{align*}
		\begin{bmatrix}[rrr|r]
		1 & 2 & 2 & 5\\
		1 & k & 3 & 7\\
		1 & 11 & k & 11\\
		\end{bmatrix}&
		\begin{bmatrix}[r]
		R_1\\ R_2\\ R_3\\
		\end{bmatrix}\\
		%
		\begin{bmatrix}[r]
		R_1\\
		R_2 - R_1\\
		R_3 - R_1\\
		\end{bmatrix}
		\begin{bmatrix}[rrr|r]
		1 & 2 & 2 & 5\\
		0 & k-2 & 1 & 2\\
		0 & 9 & k-2 & 6\\
		\end{bmatrix}&
		\begin{bmatrix}[r]
		R_4\\ R_5\\ R_6\\
		\end{bmatrix}\\
		%
		\begin{bmatrix}[r]
		R_4\\
		R_5\\
		R_6 - \\
		\end{bmatrix}
		\begin{bmatrix}[rrr|r]
		\end{bmatrix}&
		\begin{bmatrix}[r]
		R_7\\ R_8\\ R_9\\
		\end{bmatrix}
		\end{align*}
		% 2.82.c.ii
		\item [(ii)]
		% 2.82.c.iii
		\item [(iii)]
		\end{enumerate}
	\end{enumerate}
\medskip
\subsection{2.83}
% 2.83
\item [2.83.] Determine whether or not each system has a nonzero solution.
	\begin{enumerate}
	% 2.83.a	
	\item
	\sysdelim{.}{.}\systeme[xyz]{x+3y-2z=0,x-8y+8z=0,3x-2y+4z=0}
	\begin{align*}
	\begin{bmatrix}[rrr|r]
	1 & 3 & -2 & 0\\
	1 & -8 & 8 & 0\\
	3 & -2 & 4 & 0\\
	\end{bmatrix}&
	\begin{bmatrix}[r]
	R_1\\ R_2\\ R_3\\
	\end{bmatrix}\\
	%
	\begin{bmatrix}[r]
	R_1\\ R_3\\ R_2\\
	\end{bmatrix}
	\begin{bmatrix}[rrr|r]
	1 & 3 & -2 & 0\\
	3 & -2 & 4 & 0\\
	1 & -8 & 8 & 0\\
	\end{bmatrix}&
	\begin{bmatrix}[r]
	R_4\\ R_5\\ R_6\\
	\end{bmatrix}\\
	%
	\begin{bmatrix}[r]
	R_4\\
	R_5 - 3R_4\\
	R_6 - R_4\\
	\end{bmatrix}
	\begin{bmatrix}[rrr|r]
	1 & 3 & -2 & 0\\
	0 & -11 & -2 & 0\\
	0 & -11 & 6 & 0\\
	\end{bmatrix}&
	\begin{bmatrix}[r]
	R_7\\ R_8\\ R_9\\
	\end{bmatrix}\\
	%
	\begin{bmatrix}[r]
	R_7\\
	R_8\\
	R_9 + R_8\\
	\end{bmatrix}
	\begin{bmatrix}[rrr|r]
	1 & 3 & -2 & 0\\
	- & -11 & -2 & 0\\
	0 & 0 & 4 & 0\\
	\end{bmatrix}&
	\begin{bmatrix}[r]
	R_{10}\\ R_{11}\\ R_{12}\\
	\end{bmatrix}\\
	6z&=0\\
	z&=0\\
	-11y-2(0)&=0\\
	y&=0\\
	x+3(0-2(0)&=0\\
	x&=0
	\end{align*}
	Since x, y, and z are all zero, this linear system has no nonzero solutions.
	\medskip
	
	% 2.83.b
	\item
	\sysdelim{.}{.}\systeme[xyz]{x+3y-2z=0,2x-3y+z=0,3x-2y+2z=0}
	
	\medskip	
	%2.83.c
	\item
	\sysdelim{.}{.}\systeme[xyzt]{x+2y-5z+4t=0,2x-3y+2z+3t=0,4x-7y+z-6t=0}
	
	\medskip	
	\end{enumerate}
\subsection{2.86}
% 2.86
\item [2.86.] Reduce A to echelon form and then to row canonical form.
	\begin{enumerate}
	% 2.86.a	
	\item
	\begin{align*}
	%\setlength\arraycolsep{5pt} % Changes column padding
	A = 
	\begin{bmatrix*}[r]
	1 & 2 & -1 & 2 & 1\\
	2 & 4 & 1 & -2 & 3\\
	3 & 6 & 2 & -6 & 5\\
	\end{bmatrix*}
	\end{align*}
	% 2.86.b
	\item
	\begin{align*}
	A = 
	\begin{bmatrix*}[r]
	2 & 3 & -2 & 5 & 1\\
	3 & -1 & 2 & 0 & 4\\
	4 & -5 & 6 & -5 & 7\\
	\end{bmatrix*}
	\end{align*}
	\end{enumerate}
\subsection{2.88}
% 2.88
\item [2.88.] Using only 0s and 1s, list all possible 2 x 2 matrices in row canonical form.
\subsection{2.89}
% 2.89
\item [2.89.] Using only 0s and 1s, list all possible 3 x 3 matrices in row canonical form.
\subsection{2.92}
% 2.92
\item [2.92.] Consider the following system of unknowns x and y:
\[ \sysdelim{.}{.}\systeme[xy]{ax+by=1,cx+dy=0} \]
Show that if $ad-bc \neq 0$, then the system has the unique solution
\[ x=\frac{d}{ad-bc}, \quad y=-\frac{c}{ad-bc} \]
And show that if $ad-bc=0$ and if $c \neq 0$ or $d \neq 0$, then the system has no solution.
\end{enumerate}

%===============================================

\mychapter{4}{2018-01-18}
\section{Homework Review}
\subsection{1.76}
For 1.76 in homework, you can write each of the equations in the 2x3 matrix as an augmented matrix 6x2 and solve as normal for the 12 unknowns.\\
\subsection{2.80}
2.80(b) on homework:\\
z and t become free variables and x and y are dependent variables.\\
When writing out solution set, you want to assign z and t (or any free variables) to a new set of variables.\\
Ex:\\
Parameterization of solution set:
\begin{align*}
t &= q\\
z &= p\\
y &= 2p-2q+1\\
x &= -p+2q
\end{align*}

\section{Invertibility}
\begin{defn} A square matrix A is invertible if there is another square matrix B such that AB is the identity matrix.
\end{defn}

Ex:
\[
A = 
\begin{bmatrix}[rr]
5 & 17\\
2 & 7\\
\end{bmatrix}
\]
Inverse of A is
\[
\begin{bmatrix}[rr]
7 & -17\\
-2 & 5\\
\end{bmatrix}
\]

\[
\begin{bmatrix}[rr]
5 & 17\\ 2 & 7\\
\end{bmatrix}
\begin{bmatrix}[rr]
7 & -17\\ -2 & 5\\
\end{bmatrix}
=
\begin{bmatrix}[rr] 1 & 0\\ 0 & 1\\ \end{bmatrix}
\]

Notation:
\[ 
A^{-1} = \begin{bmatrix}[rr]
5 & 17\\
2 & 7\\
\end{bmatrix} ^{-1}
\]

Alternate method of writing matrices in 1.64\\
Find $\begin{bmatrix}[r] x\\ y\\ z\\ \end{bmatrix}$ such that
\[
\begin{bmatrix}[rrr] 1 & 2 & 4\\ 3 & 5 & -2\\ 3 & -1 & 3\\ \end{bmatrix}
\begin{bmatrix}[r] x\\ y\\ z\\ \end{bmatrix}
=
\begin{bmatrix} 9\\ -3\\ 16\\ \end{bmatrix}
\]
Solution
\[
\begin{bmatrix}[r] x\\ y\\ z\\ \end{bmatrix}
=
\begin{bmatrix}[rrr] 1 & 2 & 4\\ 3 & 5 & -2\\ 3 & -1 & 3\\ \end{bmatrix}^{-1}
\begin{bmatrix} 9\\ -3\\ 16\\ \end{bmatrix}
=
\begin{bmatrix}[r] 3\\ -1\\ 2\\ \end{bmatrix}
\]

\begin{thrm} Suppose A is an invertible n x m matrix. Then
\[ rref\begin{bmatrix}[r|r] A & I_n\\ \end{bmatrix}
=
\begin{bmatrix}[r|r] I_n & A_{-1}\\ \end{bmatrix}
\]
\end{thrm}

Fact: Every elementary row operation can be written as an invertible matrix.\\
\[
\begin{bmatrix}[rrr]
1 & 0 & 0\\ 0 & 4 & 0\\ 0 & 0 & 1\\
\end{bmatrix} ^{-1}
=
\begin{bmatrix}[rrr]
1 & 0 & 0\\ 0 & \frac{1}{4} & 0\\ 0 & 0 & 1\\
\end{bmatrix}
\]

%==================================================

\mychapter{5}{2018-01-19}
\section{Homework Review}
\subsection{1.76}
On number 1.76 in Homework 3, we need to find a better way of solving this type of system.
\[
\begin{bmatrix}[rr] 1 & 2\\ 3 & 6\\ \end{bmatrix}
\begin{bmatrix}[r] 2\\ -1\\ \end{bmatrix}
\]
Except this doesn't use distinct values. But any multiple of the vector (2,-1) will give us a (0,0) result so to get distinct values, we just multiply the vector (2,-1) by any scalar.\\

\section{Elementary Matrices/Row Operations}
\begin{defn} An elementary matrix is any matrix that can be obtained by applying exactly one elementary row operation to the identity matrix. \end{defn}

Example: Elementary or not?\\
\[
\begin{bmatrix}[rrr] 0 & 1 & 0\\ 1 & 0 & 0\\ 0 & 0 & 1\\ \end{bmatrix}
\]
Yes because it only takes one operation (swapping row 1 with row 2) to get to the identity matrix.
\[
\begin{bmatrix}[rrr] 0 & 0 & 1\\ 0 & 1 & 0\\ 1 & 0 & 0\\ \end{bmatrix}
\]
Yes for same reason as first problem.
\[ 
\begin{bmatrix}[rrr] 0 & 1 & 0\\ 0 & 0 & 1\\ 1 & 0 & 0\\ \end{bmatrix}
\]
No because it will take more than one elementary row operation to get the identity matrix.\\
Other elementary row operations include multiplying or subtracting/adding rows in order to get the identity matrix.\\
\section{LDU Decomposition}

Ex.\\
\[
\begin{bmatrix}[rrr]
-3 & 12 & 0\\
0 & 2 & 4\\
9 & -36 & 4\\
\end{bmatrix}
=
\begin{bmatrix}[rrr]
1 & 0 & 0\\
0 & 1 & 0\\
-3 & 0 & 1\\
\end{bmatrix}
\begin{bmatrix}[rrr]
-3 & 0 & 0\\
0 & 2 & 0\\
0 & 0 & 4\\
\end{bmatrix}
\begin{bmatrix}[rrr]
1 & -4 & 0\\
0 & 1 & 2\\
0 & 0 & 1\\
\end{bmatrix}
\]
The three matrices on the right half above have names. From left to right: Lower Triangular, Diagonal, Upper Triangular. This is where the LDU comes from.\\

%===============================================

\mychapter{6}{2018-01-22}
\section{nth Power of Matrices}
\begin{defn}
	The $n^{th}$ power of a square matrix $A$ is either the identity matrix $I$ (if $n=0$) or $AA^{n-1}$ if $n \geq 1$.
\end{defn}
Example:
\begin{align*}
	A= \begin{bmatrix}[rr] 5 & 1\\ 0 & 2\\ \end{bmatrix}\\
	A^0 = \begin{bmatrix}[rr] 1 & 0\\ 0 & 1\\ \end{bmatrix}\\
	A^2 = AA^{2-1} = 
	\begin{bmatrix}[rr] 25 & 15\\ 0 & 4\\ \end{bmatrix}\\
\end{align*}
$A^0$ will be the base case for mathematical induction.\\
Question 2 on quiz:\\
\[ A= \begin{bmatrix}[rrr] 3 & 4 & 1\\ 1 & 1 & 2\\ \end{bmatrix} \]
Find $Av=0$ where $v$ is 3 x 1. $\begin{bmatrix}[r] x\\ y\\ z\\ \end{bmatrix}$
\[ 	\begin{bmatrix}[r] 3x+4y+z\\ x+y+2z\\ \end{bmatrix}
	= \begin{bmatrix}[r] 0\\ 0\\ \end{bmatrix} \]
\[ \begin{bmatrix}[rrr|r] 1 & 0 & 7 & 0\\ 0 & 1 & -5 & 0\\ \end{bmatrix} \]
$x=-7t,y=5t,z=t$.\\

\section{Mathematical Induction}
Prove that $\sum_{k=1}^{n}k = \frac{n(n+1)}{2}$.
\begin{proof}
We proceed by induction.\\
Let $n=1$, then $\frac{1(1+1)}{2} = 1$ so the formula holds for $n=1$.\\
Assume that for some integer $k$, that $\sum_{i=1}^{k}i = \frac{k(k+1)}{2}$.\\
We show that $\sum_{i=1}^{k+1}i = \frac{(k+1)(k+2)}{2}$.\\
You know how the rest goes so finish it.
\end{proof}

\section{Vector Spaces and Scaling}
Scalar = Element of a Field\\
\begin{defn} A field is a set $\mathbb{F}$ equipped with binary operations $+$ ("addition"), $\cdot$ ("multiplication") and elements $0 \neq 1$ such that all of the following hold:
\begin{enumerate}
	\item[(i)] $a+b=b+a$
	\item[(ii)] $a+(b+c)=(a+b)+c$
	\item[(iii)] $a+0=a$
	\item[(iv)] For every $a \in \mathbb{F}$, there is $b \in \mathbb{F}$ such that $a+b=0$.
	\item[(v)] $a \cdot b = v \cdot a$
	\item[(vi)] $(a \cdot b) \cdot c = a \cdot (b \cdot c)$
	\item[(vii)] $a \cdot 1 = a$
	\item[(viii)] For every $a \neq 0$, there is $b \in \mathbb{F}$ such that $a \cdot b = 1$.
	\item[(ix)] $a \cdot (b + c) = a \cdot b + a \cdot c$ for all $a, b, c \in \mathbb{F}$.
\end{enumerate}
\end{defn}
Example:\\
$\mathbb{R}$ is the field of all real numbers.\\
$\mathbb{Q}$ is the field of rational numbers $\{\frac{a}{b}: a,b,integers,b \neq 0\}$.\\
$\mathbb{C}$ is the field of all complex numbers $\{a+ib: a,b\in \mathbb{R}\}$.

%==============================================

\mychapter{7}{2018-01-23}
\section{Homework Review}
\subsection{3.90}
Homework \#3.90 page 146\\
Let $B= \begin{bmatrix}[rrr] 1 & 8 & 5\\ 0 & 9 & 5\\ 0 & 0 & 4\\ \end{bmatrix}$. Find a triangular matrix $A$ with positive diagonal entries such that $A^2=B$.\\
Assume that \[ A= \begin{bmatrix}[rrr] a_1 & a_2 & a_3\\ 0 & a_4 & a_5\\ 0 & 0 & a_6\\ \end{bmatrix} \]
Then
\[ A^2 = \begin{bmatrix}[rrr]
	a_1^2 & a_1a_2+a_2a_4 & a_1a_3+a_2a_5+a_3a_6\\
	0 & a_4^2 & a_4a_3+a_4a_5\\
	0 & 0 & a_6^2\\
	\end{bmatrix}
	=
	\begin{bmatrix}[rrr] 1 & 8 & 5\\ 0 & 9 & 5\\ 0 & 0 & 4\\ \end{bmatrix} \]
\begin{align*}
	a_1 &= 1\\ 
	a_2 &=\\
	a_3 &=\\
	a_4 &= 3\\
	a_5 &=\\
	a_6 &= 2\\
\end{align*}
We chose these values 1, 3, and 2 because looking at the matrix, these values squared gives us our desired result.

\section{Vector Spaces}
\begin{defn}
	Let $\mathbb{F}$ be a field. A vector space over $\mathbb{F}$ is a set $V$ equipped with a binary operation $+$ (vector addition) and a function $\mathbb{F} x V \rightarrow V$ (scalar multiplication) and an element $\vec{o} \in V$ such that all of the following hold:
	\begin{enumerate}
	\item $\vec{v} + \vec{w} = \vec{w} + \vec{v}$
	\item $(\vec{u} + \vec{v}) + \vec{w} = \vec{u} + (\vec{v} + \vec{w})$
	\item $\vec{o} + \vec{v} = \vec{v}$
	\item For every $\vec{v} \in V$, there is $\vec{w} \in V$ such that $\vec{v} + \vec{w} = \vec{o}$
	\item $(\lambda + \mu)\vec{v} = \lambda \vec{v} + \mu \vec{v}$
	\item $\lambda (\vec{v} + \vec{w}) = \lambda \vec{v} + \lambda \vec{w}$
	\item $(\lambda \mu) \vec{v} = \lambda(\mu \vec{v})$
	\item $1\vec{v} = \vec{v}$
	\end{enumerate}
	In 1-3, all $\vec{u}, \vec{v}, \vec{w} \in V$ and in 5-8, all $\lambda, \mu \in \mathbb{F}$ and all $\vec{v}, \vec{w} \in V$.\\
	$\mathbb{F}$ is the field of scalars (usually $\mathbb{R}$, $\mathbb{C}$, or $\mathbb{Q}$). Thus $\lambda$ and $\mu$ are scalars.
\end{defn}
\begin{ex}
$V= \mathbb{R}^n =$ all column vectors with $n$ entries from $\mathbb{R}$.\\
$+$ is the usual vector addition.\\
$\cdot$ is the usual scalar multiplication.\\
Prove that $(\mathbb{R}^n,+,\cdot)$ is a vector space.\\
Proof that vector addition is commutative.
\begin{proof}
	Let $\vec{v}, \vec{w} \in V$. We show that $\vec{v} + \vec{w} = \vec{w} + \vec{v}$.\\
	By definition of $V=\mathbb{R}^n$, there exists $a_1,a_2,...,a_n$ and $b_1,b_2,...,b_n \in \mathbb{R}^\prime$ such that $\vec{v} = \begin{bmatrix}[c] a_1\\ a_2\\ \vdots\\ a_n\\ \end{bmatrix}$ and $\vec{w} = \begin{bmatrix}[c] b_1\\ b_2\\ \vdots\\ b_n\\ \end{bmatrix}$
	Notice $\vec{v} + \vec{w} = \begin{bmatrix}[c] a_1+b_1\\ a_2+b_2\\ \vdots\\ a_n+b_n\\ \end{bmatrix} = \begin{bmatrix}[c] b_1+a_1\\ b_2+a_2\\ \vdots\\ b_n+a_n\\ \end{bmatrix} = \vec{w} + \vec{v}$. 
\end{proof}
\end{ex}

\begin{ex}
Example of something that is not a vector space.\\
$V=\Z$ is not a vector space. Since $\Z$ isn't a field, there isn't a scalar field. It also doesn't have any reciprocals. Multiplicative inversion is not defined in $\Z$.\\
For example. $2 \in \Z$, but there is not a single element $b \in \Z$ such that $2b = 1$.
\end{ex}

\begin{defn}
Suppose $V$ is a vector space under the operations $(+,\cdot)$. A subset $W \subset V$ is a vector subset if it is a vector space under $(+,\cdot)$. 
\end{defn}
\begin{ex}
$V=\mathbb{R}^3$, $W= \{\begin{bmatrix}[c] x\\ y\\ z\\ \end{bmatrix} : z=0 \} = \{ \begin{bmatrix}[c] x\\ y\\ z\\ \end{bmatrix} : x,y \in \mathbb{R} \}$\\
Is $W$ a vector space?
\end{ex}

\begin{thrm}
The subspace criterion:\\
Suppose $V$ is a vector subspace and $W \subset V$.\\
Then $W$ is a vector subspace iff all of the following hold:
\begin{enumerate}
\item $W \neq \emptyset$
\item $\vec{v} + \vec{w} \in W$ for all $\vec{v},\vec{w} \in W$
\item $\lambda \vec{v} \in W$ for all $\lambda \in \mathbb{F}$ and all $\vec{v} \in W$
\end{enumerate}
\end{thrm}

%===========================================



\mychapter{8}{2018-01-25}
\section{Homework Review}
\subsection{3.99}
Problem 3.99 in homework:\\
We need to solve the unknowns such that $M^TM$ is equal to the identity matrix.
	\begin{align*}
	M &= \begin{bmatrix}[r] R_1\\ R_2\\ R_3\\ \end{bmatrix}\\
	MM^T &= \begin{bmatrix}[r] R_1\\ R_2\\ R_3\\ \end{bmatrix}
	\begin{bmatrix}[rrr] R_1 & R_2 & R_3\\ \end{bmatrix}\\
	&=
	\begin{bmatrix}[rrr]
	R_1R_1^T & R_1R_2^T & R_1R_3^T\\
	R_2R_1^T & R_2R_2^T & R_2R_3^T\\
	R_3R_1^T & R_3R_2^T & R_3R_3^T\\
	\end{bmatrix}\\
	&=
	\begin{bmatrix}[rrr]
	R_1 \cdot R_1 & R_1 \cdot R_2 & R_1 \cdot R_3\\
	\vdots & \vdots & \vdots\\
	\vdots & \vdots & \vdots\\
	\end{bmatrix}\\
	&=
	\begin{bmatrix}[rrr]
	1 & 0 & 0\\
	0 & 1 & 0\\
	0 & 0 & 1\\
	\end{bmatrix}
	\end{align*}
\subsection{3.96}
For problem 3.96.a:\\
\begin{proof}
Assume A and B are symmetric. Notice that $(A+B)^T=A^T+B^T = A + B$ by the law of transposes. Thus, $A+B$ is also symmetric.
\end{proof}

\section{Subspaces}
\begin{ex} V is any vector space implying $\{\vec{0}\}$ and $V$ are vector subspaces.\\
Confirm that $\{\vec{0}\}$ satisfies the subspace criterion.\\
Notice $\{\vec{0}\} \neq \{\}$. Also $\vec{0} + \vec{0} = \vec{0}\in \{\vec{0}\}$ implying closure under addition.\\
Also $\lambda\vec{0} = \vec{0}$ for all $\lambda$ implying closure under multiplication.
\end{ex}
\begin{thrm}
Every subspace of $\mathbb{R}^3$ is either
	\begin{enumerate}
	\item [0-D] The zero subspace $\{\begin{bmatrix}[r] 0\\ 0\\ 0\\ \end{bmatrix} \}$
	\item [1-D] A line passing through $\begin{bmatrix}[r] 0\\ 0\\ 0\\ \end{bmatrix}$
	\item [2-D] A plane passing through $\begin{bmatrix}[r] 0\\ 0\\ 0\\ \end{bmatrix}$
	\item [3-D] All of $\mathbb{R}^3$
	\end{enumerate}
\end{thrm}
\begin{ex}
$\{\begin{bmatrix}[r] x\\ y\\ z\\ \end{bmatrix} : x^2+y^2+z^2=1 \}$ is a sphere. It is a subset of $\mathbb{R}^3$ but is not a vector subspace. Vector subspaces proposed in the theorem above are all flat objects.
\end{ex}
\begin{thrm}
Let $A$ be an m x n matrix, and let $V$ be the set of all n-tuples (as columns) $\vec{V}$ such that $A\vec{v} = \vec{0}$. Then $V$ is a vector subspace of $\mathbb{R}^n$.\\
$A$ is (m x n), $\vec{v}$ is (n x 1).
\end{thrm}
\begin{proof}
Use the subspace criterion. (page 159 Theorem 4.2)\\
\begin{enumerate}
	\item Notice $A\vec{0}_n = \vec{0}_m$. Thus $\vec{0}_n \in V$.
	\item Assume $\vec{v}, \vec{w} \in V$. We must show $\vec{v} + \vec{w} \in V$.\\
	Notice $A(\vec{v} + \vec{w}) = A\vec{v} + A\vec{w} = \vec{0} + \vec{0} = \vec{0}$ by distributive law of matrix multiplication. Thus $V$ is closed under addition.
	\item Assume $\lambda \in \mathbb{R}$ and $\vec{v} \in V$. We must show $\lambda\vec{v} \in V$.\\
	Notice $A(\lambda\vec{v}) = \lambda(A\vec{v}) = \lambda\vec{0} = \vec{0}$ by property of matrix algebra. 
\end{enumerate}
\end{proof}

%====================================================

\mychapter{9}{2018-01-26}
\section{Nullspaces}
\begin{defn}
Let $A$ be an m x n matrix. The nullspace of $A$ is the set of all vectors $\vec{v}$ such that $A\vec{v} = \vec{0}$.\\
Notation: $N(A) = null(A) = nulspace(A) = \{\vec{v}: A\vec{v} = \vec{0}\}$
\end{defn}

\begin{defn}
$M$ is orthogonal is $MM^T = I$.
\end{defn}

\section{Linear Algebra Core Concepts}
Five Concepts Required for Linear Algebra:
\begin{enumerate}
\item Linear Combinations
\item Span of Set (Spanning Set)
\item Linear Independence
\item Basis of a Vector Space
\item Dimension
\end{enumerate}

\begin{defn}
Let $V$ be a vector space and $S \subset V$. A linear combination of elements of $S$ is any expression having the form $C_1\vec{v}_1+C_2\vec{v}_2+\dots+C_n\vec{v}_n$ where $C_1,C_2,...,C_n$ are scalars and $\vec{v}_1,\vec{v}_2,...,\vec{v}_n$ are in S.
\end{defn}

\begin{ex}
Write $(12,4)$ as a linear combination of $(1,0)$ and $(0,1)$.\\
Solution: $(12,4) = 12(1,0)+4(0,1)$
\end{ex}
\begin{ex}
Write $3+4x+x^2$ as a linear combination of $1$, $x$, and $x^2$.\\
Solution: $3+4x+x^2=3(1)+4(x)+1(x^2)$
\end{ex}
\begin{ex}
Write $(12,4)$ as a linear combination of $(0,1)$ and $(1,1)$.\\
Solution: $(12,4) = x(0,1) + y(1,1)$.\\
$12 = 0x+1y$, $4=1x+1y$\\
$y=12$, $x=-8$\\
$(12,4) = -8(0,1)+12(1,1)$
\end{ex}

%==========================================

\mychapter{10}{2018-01-29}
\section{Spans}
\begin{defn}
Suppose $V$ is a vector space, and $S\subset V$ is any subset. The span of $S$ is the set of all linear combinations of elements of $S$.
\[ span(S) = \{ c_1\vec{v_1}+c_2\vec{v_2}+\dots+c_n\vec{v_n}: c_1,c_2,...,c_n \in \mathbb{F}, \vec{v_1},\vec{v_2},...,\vec{v_n} \in S \} \]
If $S=\{\}$, then $span(S)=\{\vec{0}\}$
\end{defn}
\begin{ex}
$span\{(1,0),(0,1)\} =$ all of $\mathbb{R}^2$\\
Notice $(x,y)=x(1,0)+y(0,1)$
\end{ex}
\begin{ex}
$span\{(0,1,0),(0,0,1)\} \subset \mathbb{R}^3$ is the $(y,z)$ plane in $\mathbb{R}^3$.
\end{ex}
\begin{ex}
$span\{(1,0,0),(2,0,0)\}$ is the x-axis in $\mathbb{R}^3$.\\
Since one of these vectors is a scaled version of the other, then we can remove it since it is redundant. Essentially if we have multiple vectors where a single elementary row operation can make two equal, then it can be removed. 
\end{ex}
\begin{thrm}
Suppose $V$ is a vector space, and $S \subset V$ is any subset. Then the span of $S$ is a vector subspace of $V$.
\end{thrm}
\begin{proof}
Use the subspace criterion.\\
Special case $S=\{\} \Rightarrow span(S)=\{\vec{0}\}$ the zero subspace.\\
Assume $S$ is not empty.
	\begin{enumerate}
	\item Notice $\vec{0} = 0\vec{v}$ for any $\vec{v} \in S$\\
	$\Rightarrow \vec{0} \in span(S)$.
	\item We use a shortcut to combine parts two and three of the subspace criterion.\\
	Assume $\lambda \in \mathbb{F}$ and $\vec{v},\vec{w} \in span(S)$. Prove $\lambda\vec{v}+\vec{w} \in span(S)$.
	\end{enumerate}
\end{proof}

\section{Spanning Set}
\begin{defn}
Suppose $V$ is a vector space and $W$ is a subspace. A spanning set for $W$ is a subset $S$ such that $span(S) = W$.
\end{defn}
\begin{ex}
Does the given set span $\mathbb{R}^3$?\\
$S_1=\{(1,0,0),(0,1,0),(0,0,1)\}$ YES\\
$S_2=\{(1,0,0),(0,1,0)\}$ NO (only two vectors)\\
$S_3=\{(1,0,0),(0,1,0),(0,0,1),(1,1,1)\}$ YES\\
$S_4=\{(1,0,0),(1,1,0),(1,1,1)\}$ YES\\
$S_4:(x,y,x)=(x-y)(1,0,0)+(y-z)(1,1,0)+z(1,1,1)$\\
$S_5=\{(1,2,3),(5,6,7)\}$ NO (only two vectors)\\
$S_6=\{(1,0,0),(0,1,0),(0,0,0)\}$ NO\\
$S_7=\{(0,1,1),(1,0,1),(1,1,0)\}$ YES (tip: write out as matrix where each vector is a column then do rref)\\
$S_8=\{(0,1,-1),(1,0,-1),(1,-1,0)\}$ NO
\end{ex}

%=================================================

\mychapter{11}{2018-01-30}
\section{Row and Column Spaces}
\begin{defn}
Suppose $A$ is an m x n matrix. The row space of $A$ is the span of the rows of $A$. The column space of $A$ is the span of the columns of $A$.
\end{defn}
\begin{ex}
\[ \begin{bmatrix}[rrrrr] 1 & 2 & 3 & 0 & 6\\ 0 & 0 & 0 & 1 & 7\\ \end{bmatrix} \]
\[ colspace(A) = span\{\begin{bmatrix}[r]1\\0\\ \end{bmatrix}, \begin{bmatrix}[rr]2\\0\\ \end{bmatrix}, ...\} \subset \mathbb{R}^2 = span\{\begin{bmatrix}[r]1\\0\\ \end{bmatrix}, \begin{bmatrix}[r]0\\1\\ \end{bmatrix} \} \]
\[ rowspace(A) = \{\begin{bmatrix}[rrrrr]1&2&3&0&6\\ \end{bmatrix},\begin{bmatrix}[rrrrr]0&0&0&1&7\\ \end{bmatrix}\} \subset \mathbb{R}^5 \]
In our colspace, we were able to reduce our span with elementary row operations but in our rowspace we aren't able to reduce it any further.
\end{ex}
Our previous example can be written as the following: $\sysdelim{.}{.}\systeme[x_1x_2x_3x_4x_5]{x_1+2x_2+3x_3+6x_5=0,x_4+7x_6=0}$\\
Recall $nullspace(A)=\{\vec{v}: A\vec{v} = 0\}$\\
\[ \vec{v} = \begin{bmatrix}[l] x_1\\x_2\\x_3\\x_4\\x_5\\ \end{bmatrix} = \begin{bmatrix}[l] -2x_2-3x_3-6x_5\\ free\\ free\\ -7x_5\\ free\\ \end{bmatrix} = \begin{bmatrix}[l] -2a-3b-6c\\a\\b\\-7c\\c\\ \end{bmatrix} = a\begin{bmatrix}[r]-2\\1\\0\\0\\0\\ \end{bmatrix}+b\begin{bmatrix}[r]-3\\0\\1\\0\\0\\ \end{bmatrix} + c\begin{bmatrix}[r]-6\\0\\0\\-7\\1\\ \end{bmatrix}\]
Spanning set for null space of $A$:
\[ s= \{\begin{bmatrix}[r]-2\\1\\0\\0\\0\\ \end{bmatrix}, \begin{bmatrix}[r]-3\\0\\1\\0\\0\\ \end{bmatrix},\begin{bmatrix}[r]-6\\0\\0\\-7\\1\\ \end{bmatrix} \} \]
To tell if you can reduce vectors, look at the zero rows. Since multiplying by a scalar will always give a zero in those rows, this means that there is no combination in which we get a nonzero value in the zero row. This means there are no redundancies. 

\section{Homework Review}
\subsection{4.36}
$(a,b)+(c,d) = (a+c,b+d)$\\
Above is a function for how to add vectors together.\\
$k(a,b)=(ka,0)$ This is the function for vector scalars.\\
Verify that this satisfies $A_4$\\
Suppose that $(a,b),(c,d)$ are given. Notice $(a,b)\vec{+}(c,d)=(a+c,b+d)$ by definition of "$\vec{+}$". By the commutative property of addition, we can rewrite this as $(c+a,a+b) = (c,d)\vec{+}(a,b)$ by definition of "$\vec{+}$".\\
$A_1-A_4$: These hold because the addition is defined as usual.\\
To show $M_4$ is violated, find a specific $\vec{u}$ such that $1\vec{u} \neq \vec{u}$.


\mychapter{12}{2018-02-01}
\section{Homework Review}
\subsection{4.43}

Part (b)\\
$v=2t^2+3t-4$, $w=t^2-2t-3$\\
Write $u=4t^2-6t-1$ as a linear combination of $v$ and $w$ if possible.\\
$4t^2-6t-1=x(2t^2+3t-4)+y(t^2-2t-3)$\\
Equate coefficients:
\begin{align*}
	t^2 \quad 4 &= 2x+y\\
	t \quad -6 &= 3x-2y\\
	1 \quad -1 &= -4x-3y\\
	\begin{bmatrix}[rr|r]
	2&1&4\\3&-2&-6\\4-&-3&-1\\
	\end{bmatrix}
	&= \begin{bmatrix}[rr|r] 1&0&0\\0&1&0\\0&0&1\\ \end{bmatrix}\\
	0x+0y&=1
\end{align*}
Thus, there is no solution to this system. Empty Solution Set (or inconsistent system).

\subsection{4.49}
Assume $V$ is a vector space and $W$, $U$, and $U \cup W$ are subspaces of $V$. Show $U \subset W$ or $W \subset U$.\\
\begin{proof}
Proof by Contradiction.\\
Assume $U \not\subset W$ and $W \not\subset U$. Then $V=\mathbb{R}^2$, $U=\{(x,y):y=0\}$, and $W=\{(x,y):x=0\}$. Notice that $U \cup W$ is not a subspace of $V$. Notice that $(1,0)\in U$ and $(0,1) \in W$ but $(1,0)+(0,1)=(1,1) \not\in U \cup W$. [TO BE COMPLETED. THIS WONT BE ON TEST]
\end{proof}

\subsection{4.51}
(a) Solve the system:\\
\[ (x,y,z)=C_1(1,1,1) + C_2(0,1,2)+C_3(0,1,3) \]
$x=C_1$, $y=C_1+C_2+C_3$, and $z=C_1+2C_2+3C_3$
\begin{align*}
\begin{bmatrix}[r]x\\y\\z\\ \end{bmatrix}
= \begin{bmatrix}[rrr]1&0&0\\1&1&1\\1&2&3\\ \end{bmatrix}
\begin{bmatrix}[r]C_1\\C_2\\C_3\\ \end{bmatrix}
\end{align*}

\subsection{4.52}
(a) 
$\sysdelim{.}{.}\systeme[C_1C_2C_3abc]{C_1+C_2+C_3 = a, C_1+2C_2+C_3 =b, C_1+3C_2+2C_3 = c}$
\begin{align*}
rref \begin{bmatrix}[rrr|r]
1&1&0&a\\ 1&2&1&b\\ 1&3&2&c\\
\end{bmatrix}
= \begin{bmatrix}[rrr|r] 1&1&0&a\\0&1&1&b-a\\0&2&2&c-a\\ \end{bmatrix}
=
\begin{bmatrix}[rrr|r] 1&1&0&a\\0&1&1&b-a\\0&0&0&a-2b+c\\ \end{bmatrix}
\end{align*}
Since we end with an equation $0 = a-2b+c$ , then only few vectors work as a solution.

\section{Zero Vector Lemma}
\begin{lemma} In any vector space, there is only one zero vector. \end{lemma}

% =========================================

\mychapter{13}{2018-02-05}
\section{Exam Review}
\subsection{\#3}
$x+7y-5z=0$. Find a finite spanning set for the solution space.
\[ \begin{bmatrix}[rrr|r] 1&7&-5&0\\ \end{bmatrix} \]
x is dependent and y and z are free variables.\\
Parametrization of Solution Set:
\[ \begin{bmatrix}[r] x\\y\\z\\ \end{bmatrix} = \begin{bmatrix}[r] -7y+5z\\free\\free\\ \end{bmatrix} = \begin{bmatrix}[r] -7s+5t\\s\\t\\ \end{bmatrix} \]
Rewrite vector as linear combination:
\[ =s \begin{bmatrix}[r]-7\\1\\0\\ \end{bmatrix} +t \begin{bmatrix}[r] 5\\0\\1\\ \end{bmatrix} \]
Spanning Set = $\{\begin{bmatrix}[r]-7\\1\\0\\ \end{bmatrix} , \begin{bmatrix}[r]5\\0\\1\\ \end{bmatrix} \}$

\subsection{\#6b}
Skew symmetric means that it is the negative of its transpose.\\
Notice: $M=\frac{1}{2}M + \frac{1}{2}M + \frac{1}{2}M^T-\frac{1}{2}M^T = \frac{1}{2}(M+M^T)+\frac{1}{2}(M-M^T)$

\section{Homework Review}
\subsection{5.50(a)}
Consider the solution space
\[ C_1(1,2,-3,1) + C_2(3,7,1,-2)+C_3(1,3,7,-4) = (0,0,0,0) \]
\begin{align*}
rref
\begin{bmatrix}[rrr|r]
1&3&1&0\\2&7&3&0\\-3&1&7&0\\1&-2&-4&0\\ \end{bmatrix}
&= \begin{bmatrix}[rrr|r]
1&0&-2&0\\ 0&1&1&0\\0&0&0&0\\0&0&0&0\\ \end{bmatrix}\\
C_1 &= 2\\ C_2 &= -1\\ C_3 &= 1\\
\end{align*}
Now we verify our results:
\[ 2(1,2,-3,1)+(-1)(3,7,1,-2)+1(1,3,7,-4) = (0,0,0,0) \]
When looking at this equation, we can see that there is a linear independence. We can see this easier by moving the sets around to show that one of the sets is in the span of the others.
\[ (3,7,1,-2) = 2(1,2,-3,1)+1(1,3,7,-4) \]

\section{Linear Dependence/Independence}
\begin{defn}
Let $V$ be a vector space. Then a subset $S=\{\vec{v_1},\vec{v_2},...,\vec{v_n}\}$ is linearly dependent if the system $C_1\vec{v_1}+C_2\vec{v_2}+\dots+C_n\vec{v_n} = \vec{)}$ has only the trivial solution $C_1 = C_2 = \dots = C_n = 0$.
\end{defn}
\begin{defn}
A set $S=\{\vec{v_1},\vec{v_2},...,\vec{v_n}\}$ is linearly independent if one of the vectors in $S$ lies in the span of the others.
\end{defn}
\begin{ex}
Dependent or Independent?\\
\begin{align*}
S_1&=\{(1,0,0),(0,1,0),(0,0,1)\}\\
S_2&=\{(1,0,0),(0,1,0)\}\\
S_3&=\{(1,2,0),(3,6,0)\}\\
S_4&=\{(1,0,0),(0,1,0),(0,0,1),(1,2,3)\}\\
S_5&=\{(1,0,-1),(1,-1,0),(0,1,-1)\}
\end{align*}
Start by writing down each system.
\[ S_1: \quad C_1(1,0,0)+C_2(0,1,0)+C_3(0,0,1)=(0,0,0) \]
How many solutions does this have?\\
Simplifying the left side of this equation gives us $(C_1,C_2,C_3)=(0,0,0)$. This has a unique solution therefore it is independent.
\begin{align*}
S_1: &I\\ S_2: &I\\ S_3: &D\\ S_4: &D\\ S_5: &D
\end{align*}
Another way to solve this using rref:
\[ rref \begin{bmatrix}[rrr]1&0&-1\\1&-1&0\\0&1&-1\\ \end{bmatrix} = \begin{bmatrix}[rrr] 1&0&-1\\0&1&-1\\0&0&0\\ \end{bmatrix} \]
Since this produced a zero row, we get linear dependence.\\
Another example: $S_6=\{(1,2,3),(0,0,0)\}$. This one is dependent since the first vector can easily be written as a multiple of the second vector.
\end{ex}

\mychapter{14}{2018-02-06}
\section{Homework Review}
\subsection{4.63}
Prove $colspace(AB)\subset colspace(A)$.
\begin{proof}
Assume that $\vec{v} \in colspace(AB)$. Then there exists $\vec{w}$ such that $AB\vec{w} = \vec{v}$.


Thus $\vec{v} \in colspace(A)$. 
\end{proof}

\section{Target and Range}
\begin{defn}
(See discussion on P294 - P295)\\
Let $A$ be an n x m matrix. The range of $A$, also known as the image, is the set of all vectors $\vec{v}$ such that there exist $\vec{w}$ with $A\vec{w} = \vec{v}$. 
\end{defn}
\begin{ex}
$A = \begin{bmatrix}[rrr] 1&3&2\\0&1&0\\ \end{bmatrix}$\\
Define $f(\vec{w}) = A\vec{w}$. Domain: $\mathbb{R}^3$, Target: $\mathbb{R}$, Range: $\mathbb{R}^2$.\\

Notice $f(\begin{bmatrix}[r]x\\y\\z\\ \end{bmatrix}) = \begin{bmatrix}[rrr]1&3&2\\0&1&0\\ \end{bmatrix}\begin{bmatrix}[r]x\\y\\z\\ \end{bmatrix} = \begin{bmatrix}[r]x+3y+2z\\y\\ \end{bmatrix}$.\\
Is $\begin{bmatrix}[r]0\\0\\\end{bmatrix}$ in the range of $f$? Yes it is $(0,0,0)$. How about $\begin{bmatrix}[r]1\\0\\\end{bmatrix}$? Yes it is $(1,0,0)$ because plugging in $f(\begin{bmatrix}[r]1\\0\\0\\\end{bmatrix})$ into our result for $f(\begin{bmatrix}[r]x\\y\\z\\\end{bmatrix})$ gives us $(1,0,0)$.There are infinite vectors in the range of $f$ because it is an infinite plane. 
\end{ex}
\begin{thrm}
Let $A$ be an m x n matrix. Then the range of $A$ us the column space.
\end{thrm}
\begin{ex}
$A=\begin{bmatrix}[rr]1&0\\2&1\\3&0\\\end{bmatrix}$. $f(\vec{v} = A\vec{v}$. This can be written as$f(\begin{bmatrix}[r]x\\y\\\end{bmatrix} = \begin{bmatrix}[r]x\\2x+y\\3x\\\end{bmatrix}$. So $\begin{bmatrix}[r]0\\1\\0\\\end{bmatrix} = f(\begin{bmatrix}[r]0\\1\\\end{bmatrix})$.\\
Then $Domain(f)=\mathbb{R}^2$, $Target(f)=\mathbb{R}^3$, and $Range(f)= spam\{\begin{bmatrix}[r]0\\1\\0\\\end{bmatrix},\begin{bmatrix}[r]1\\2\\3\\\end{bmatrix}\}$. This mean that the range is also isomorphic to $\mathbb{R}^2$.\\
$\begin{bmatrix}[r]1\\0\\1\\\end{bmatrix}$ is not in the range of $f$ since there is no value for $x$ such that $x=1$ and $3x=1$.\\

Another way of looking at this is:
\[ f(\begin{bmatrix}[r]x\\y\\\end{bmatrix}) = \begin{bmatrix}[r]x\\2x+y\\3x\\\end{bmatrix} = \begin{bmatrix}[r]a\\b\\c\\\end{bmatrix} \]
\begin{align*}
x&=a\\ 2x+y&=b\\ 3x&=c\\
\begin{bmatrix}[rrr]1&0&a\\2&1&b\\3&0&c\\\end{bmatrix} &\rightarrow \begin{bmatrix}[rr|r]1&0&a\\0&1&b-2a\\0&0&c-3a\\\end{bmatrix}
\end{align*}
\end{ex}
\begin{ex}
Show that $P_1=1$, $P_2=1+t$, and $P_3=1+t+t^2$ are linearly independent.\\
Show that $C_1P_1+C_2P_2+C_3P_3=0$ has exactly one solution $C_1=C_2=C_3=0$.\\
The quickest way of doing this is to write the coefficient matrix and find a dependency from that.
\[ \begin{bmatrix}[rrr]1&0&0\\1&1&0\\1&1&1\\\end{bmatrix} \]
We have to show that these rows are linearly independent.\\
\[ rref\begin{bmatrix}[rrr]1&0&0\\1&1&0\\1&1&1\\\end{bmatrix} \rightarrow \begin{bmatrix}[rrr]1&0&0\\0&1&0\\0&0&1\\\end{bmatrix} \]
Think: Can we write the last row as a linear combination of the other rows? Think the same for the other rows.\\
In this example, we have no redundant rows and no zero rows therefore we have linear independence.\\
Another way of solving this problem:\\
Think of each of $P_1$, $P_2$, and $P_3$ as a column of every real number. So for $P_1$, every single entry equals 1. For $P_2$, there are infinitely many entries for $1+t$, and the same goes for $P_3$.\\
\begin{center}
\begin{tabular}{c|ccc}
t&$P_1$&$P_2$&$P_3$\\
0&1&1&1\\
1&1&2&3\\
2&1&3&7\\
3&1&4&13\\
\end{tabular}
\end{center}
\begin{align*}
rref\begin{bmatrix}[rrrr]1&1&1&1\\1&2&3&4\\1&3&7&13\\\end{bmatrix}
=\begin{bmatrix}[rrrr]1&0&0&*\\0&1&0&*\\0&0&1&*\\\end{bmatrix}
\end{align*}
Thus since we get an identity matrix, this system is independent.
\end{ex}


\mychapter{14}{2018-02-08}
\section{Homework Review}
\subsection{5.58}
Assume $Au_1,Au_2,...,Au_r$ are linearly independent where $A$ is a matrix.\\
Suppose $c_1,c_2,...,c_r$ are scalars such that $c_1\vec{u_1}+c_2\vec{u_2}+\dots+c_r\vec{u_r} = \vec{0}$.\\
We are still missing something here. We need to include something about the matrix A and matrix multiplication (multiply both sides by A).
Hence $c_1=c_2=c=\dots=c_r=0$.
[At very end] Therefore, by definition, $u_1,u_2,...,u_r$ are linearly independent.

\subsection{5.62.a}
\[ \sysdelim{.}{.}\systeme[xyz]{x+3y+2z=0,x+5y+z=0,3x+5y+8z=0} \]
See homework 6 for the rref of this system.\\
So we get
\[ \begin{bmatrix}[r]x\\y\\z\\\end{bmatrix}=\begin{bmatrix}[r]-\frac{7}{2}z\\\frac{1}{2}z\\z\\\end{bmatrix}=\begin{bmatrix}[r]-\frac{7}{2}t\\\frac{1}{2}t\\t\\\end{bmatrix}=t\begin{bmatrix}[r]-\frac{7}{2}\\\frac{1}{2}\\1\\\end{bmatrix} \]
A basis for the solution set:
\[ S=\{\begin{bmatrix}[r]-\frac{7}{2}\\\frac{1}{2}\\1\\\end{bmatrix}\} \]
Suppose $c\in\mathbb{R}$ and $c\begin{bmatrix}[r]-\frac{7}{2}\\ \frac{1}{2}\\1\\\end{bmatrix} = \begin{bmatrix}[r]0\\0\\0\\\end{bmatrix}$.\\
This has the unique solution $c=0$.
You can also multiply this basis by any scalar and still get a valid basis.\\
See the following section for more information on bases.

\section{Basis}
\begin{defn}
Let $V$ be a vector space. Then a subset $S\subset V$ is a basis for $V$ if both of the following hold:
	\begin{enumerate}
	\item $S$ is a spanning set for $V$
	\item $S$ is linearly independent.
	\end{enumerate}
\end{defn}
\begin{thrm}
Suppose $B_1$ and $B_2$ are bases for a vector space. Then $B_1$ and $B_2$ have the same number of elements.
\end{thrm}
\begin{defn}
The dimension of a vector space $V$ is the number of elements in any basis of $V$.
\end{defn}

\section{Rank}
\begin{defn}
Let $A$ be a matrix. The row rank of $A$ is the dimension of the row space of $A$. The column rank of $A$ is the dimension of the column space of $A$.
\end{defn}
\begin{ex}
\[ M=\begin{bmatrix}[rrrr]1&0&1&1\\1&1&3&0\\0&1&2&-1\\\end{bmatrix} \]
$rowspace \subset \mathbb{R}^4$ because each row is a vector of four entries.\\
$colspace \subset \mathbb{R}^3$ because each column is a vector of three entries.\\
$rowrank(M)=2$
\[ rref(M)=\begin{bmatrix}[rrrr]1&0&1&1\\0&1&2&-1\\0&0&0&0\\\end{bmatrix} \]
Since we get linear independence and a zero row, then our row rank is 2.\\
$colrank(M)=rowrank(M^T)=2$\\
Repeating the same process as rowrank but on the transpose of the matrix gives us the column rank.
\end{ex}

\mychapter{15}{2018-02-12}
\section{Homework Review}
\subsection{5.52.b}
\[ v_1 = e^t \quad v_2 = e^{2t} \quad v_3 = t \]
When $t=0$, $\begin{bmatrix}[r]1\\1\\0\\\end{bmatrix}$, $t=1$, $\begin{bmatrix}[r]e\\e^2\\1\\\end{bmatrix}$, $t=2$, $\begin{bmatrix}[r]e^2\\e^4\\2\\\end{bmatrix}$.\\
Show $rref\begin{bmatrix}[lll]1&1&0\\e&e^2&1\\e^2&e^4&2\\\end{bmatrix}$ is the identity matrix.

\subsection{5.82.a}
Find the dimension of the space of all n x n symmetric matrices.\\
\begin{tabular}{c|c}
n & $dim(Sym^n)$\\
\hline
1 & 1\\
2 & 3\\
3 & 6\\
4 & 10\\
5\\
\end{tabular}\\
Basis for $sy^1=\{[1]\}$. Is ever 1 x 1 symmetric matrix a linear combination of this? Yes.\\
Basis for $sym^2=\{\begin{bmatrix}[rr]1&0\\0&0\\\end{bmatrix},\begin{bmatrix}[rr]0&0\\0&1\\\end{bmatrix},\begin{bmatrix}[rr]0&1\\1&0\\\end{bmatrix}\}$. \\These matrices follow the form of $\begin{bmatrix}[rr]a&c\\c&b\\\end{bmatrix}$.\\
Basis for $sym^3 = \begin{bmatrix}[rrr]a&b&c\\b&d&e\\c&e&f\\\end{bmatrix}$. This pattern is formed because of its symmetry.

\section{Examples of Bases}
These can all be described as standard bases.
\begin{ex}
The standard basis for $\mathbb{R}^n$ (n-tuples as columns)\\
$e_i = $ the 0/1 column whose ith entry is 1, 0s elsewhere.
\end{ex}
\begin{ex} The standard basis for $P_n(t)$ (polynomials in $t$ with $deg\in n$.
\[ \{1,t,t^2,t^3,...,t^{n-1},t^n\} \]
To write $t^3$ as a linear combination of all items in this set, we need to write $\{ 0,0,0,1,0,0,...,0\}$.\\
$\therefore dim(P_n(t)) = n+1$
\end{ex}
\begin{ex}
Standard basis for $M_{2,3}\mathbb{R}$ is all 2 x 3 matrices with entries in $\mathbb{R}$.\\
The dimension is 6. We get this by taking 2 times 3.
\[ \{\begin{bmatrix}[rrr]1&0&0\\0&0&0\\\end{bmatrix},\begin{bmatrix}[rrr]0&1&0\\0&0&0\\\end{bmatrix}, \begin{bmatrix}[rrr]0&0&1\\0&0&0\\\end{bmatrix}, \begin{bmatrix}[rrr]0&0&0\\1&0&0\\\end{bmatrix}, \begin{bmatrix}[rrr]0&0&0\\0&1&0\\\end{bmatrix}, \begin{bmatrix}[rrr]0&0&0\\0&0&1\\\end{bmatrix}\} \]
Fact: As a vector space, $\mathbb{R}_{2,3}$ is isomorphic to $\mathbb{R}^6$.
\end{ex}

\section{Rank Continued}
\begin{thrm}
Suppose $A$ is an m x n matrix, then the column rank of $A$ is equal to the row rank of $A$.
\end{thrm}
\begin{defn}
The rank of a matrix $A$ is the row rank of $A$ and therefore also the column rank of $A$ because of the previous theorem.
\end{defn}
\begin{lemma}
Suppose $a,b\in \mathbb{R}$ and $a\leq b$ and $b\leq a$. Then $a=b$.
\end{lemma}
\begin{proof}
We prove the previously stated theorem.\\
Let the rows of $A$ be denoted
\begin{align*}
R_1 &= (a_{1,1},a_{1,2}, ..., a_{1,n})\\
R_2 &= (a_{2,1},a_{2,2}, ..., a_{2,n})\\
\vdots\\
R_m &= (a_{m,1},a_{m,2}, ..., a_{m,n})
\end{align*}
Suppose a basis for $rowspace(A)$ is
\begin{align*}
S_1 &= (b_{1,1},b_{1,2}, ..., b_{1,n})\\
S_2 &= (b_{2,1},b_{2,2}, ..., b_{2,n})\\
\vdots\\
S_r &= (b_{r,1},b_{r,2}, ..., b_{r,n})
\end{align*}
(This is assuming that $r=rowrank(A)$. Both sets have the same span.)\\
There exist scalars $k_{i,j}$ such that
\[ R_i = \sum_{j=1}^rk_{i,j}S_j \]
This gives a system of linear equations for each column.
\[ columnrank(A) \leq rowrank(A) = columnrank(A^T) \leq rowrank(A^T) = columnrank(A) \]
By our lemma, $columnrank(A) = rowrank(A)$.
\end{proof}

\mychapter{15}{2018-02-13}
\section{Homework Review}
\subsection{5.55}
Given $A=\{u_1,u_2,u_3,...,u_m\}$, $B=\{v_1,v_2,v_3,...,v_n\}$ lie in a vector space $V$. Assume that $A\cup B$ is linearly independent. Show $span(A)\cap span(B) = \{\vec{0}\}$.
\begin{proof}
First notice $\{\vec{0}\} \subset span(A) \cap span(B)$. We still must show $span(A) \cap span(B) \subset \{\vec{0}\}$.\\
Suppose $\vec{w}\in span(A) \cap span(B)$. Since $\vec{w}\in span(A)$, there are scalars $a_1,a_2,...,a_m$ such that (by definition of span)
\[ \vec{w}=a_1u1+a_2u_2+\dots+a_mu_m \] Also since $\vec{w}\in span(B)$ there are scalars $b_1,b_2,...,b_n$ such that 
\[ \vec{w}=b_1v_1+b_2v_2+\dots+b_nv_n \]
Hence $a_1u_1+a_2u_2+\dots+a_mu_m = b_1v_1+b_2v_2+\dots+b_nv_n$.\\
Thus $a_1u_1+a_2u_2+\dots+a_mu_m+(-b_1)v_1+(-b_2)v_2+\dots+(-b_n)v_n = \vec{0}$.\\
Since $A \cup B = \{u_1,...u_m,v_1,...,v_n\}$ is linearly independent, then
\[ a_1 = a_2 = \dots = a_m = -b_1 = -b_2 = \dots = -b_n = 0 \]
\[ \Rightarrow \vec{w}=0u_1+0u_2+\dots+0u_m=\vec{0} \]
\end{proof}

\section{Intersect of Subspaces}
\begin{thrm}
Suppose $V$ is a vector space and $W_1$ and $W_2$ are vector subspaces of $V$. Then $W_1\cap W_2$ is a vector subspace of $V$.
\end{thrm}
\begin{proof}
Use the subspace criterion (Thrm 4.2 in book).
\end{proof}
\begin{ex}
$V=\mathbb{R}^3$\\
$W_1 = span\{(1,0,0),(0,1,0)\}$\\
$W_2 = span\{(0,1,0),(0,0,1)\}$\\
$W_1$ is a plane in $\mathbb{R}^3$. It has infinite solutions and its vectors are linearly independent. The plane can be expressed as $z=0$.\\
$W_2$ is also a plane just the same as $W_1$ however it can be expressed as $z=0$.\\
$W_1 \cap W_2 = span\{(0,3,0)\}$\\
There are infinite possible entries for $y$ and the intersect of these subspaces gives us a line (y-axis). The three in the final result is arbitrary.
\end{ex}

\section{Sums of Subspaces}
\begin{defn}
Suppose $V$ is a vector space and $W_1$ and $W_2$ are subsets of $V$. The sum of $W_1$ and $W_2$ is $W_1+W_2:= \{\vec{w_1}+\vec{w_2} : \vec{w_1} \in W_1, \vec{w_2} \in W_2 \}$.
\end{defn}
Look up the diamond of inclusions diagram.
\begin{ex}
$V=\mathbb{R}^2$ (ordered pairs)\\
$W_1 = \{(x,): 0\leq x \leq 3\}$\\
$W_2=\{(0,y): 0\leq y \leq 2\}$\\
These are subsets however they are not subspaces since they aren't closed under addition.\\
$W_1+W_2=\{(x,y): 0\leq x\leq 3, 0\leq y \leq 2\}$\\
This forms a rectangle bound by $(0,2),(3,2),(0,0),(3,0)$. The point $(2,1)=[(2,0)\in W_1]+[(0,1)\in W_2]\in W_1+W_2$ which shows that this point is within this region.
\end{ex}
\begin{thrm}
Let $V$ be a vector space with subspaces $W_1$ and $W_2$. Then $W_1+W_2=span(W_1 \cup W_2)$.
\end{thrm}
\begin{proof}
(exercise)
\end{proof}


\mychapter{16}{2018-02-15}
\section{Homework Review}
\subsection{5.70.a}
For each of the following matrices, find (i) columns that are linear combinations of preceding columns, and (ii) columns that form a basis for the column space:\\	
$A=\begin{bmatrix}[rrrrrr]1&1&2&2&3&3\\2&2&5&6&8&7\\1&1&4&6&7&7\\\end{bmatrix}$
		\begin{enumerate}
		%5.70.a.i		
		\item[(i)] Column 1 and 2 are the same as each other. $C_2 = 1C_1$. $\{C_1,C_3\}$ are linearly independent since they are not linear combinations of each other.\\
		Is $C_4 \in span\{C_1,C_3\}$? Yes. We need to find a linear combination that gets us $C_4$.\\
		$C_4 = (-2)C_1 + 2C_3$. Thus $C_4$ is a linear combination of $\{C_1,C_3\}$.\\
		$C_5 = C_1+C_4 = C_1+(-2)C_1+2C_3 = -C_1+2C_3$\\
		$\{C_1,C_3,C_6\}$ are linearly independent and span colspace.\\
		Thus the rank of the matrix is 3 since we found three linearly independent columns.\\
		We can also find these by taking the transpose of the matrix then running rref and finding the zero rows. 
		%5.70.a.ii		
		\item[(ii)] 	$\{C_1,C_3,C_6\}$ forms a basis since they are linearly independent and span the column space. We can also replace $C_1$ with $C_2$ to form a different basis. Or we can do the following:\\
		$\{\begin{bmatrix}[r]1\\0\\0\\\end{bmatrix},\begin{bmatrix}[r]0\\1\\0\\\end{bmatrix},\begin{bmatrix}[r]0\\0\\1\\\end{bmatrix}\}$\\
		$\mathrm{rref}(A^T)=\begin{bmatrix}[rrr]1&0&0\\0&1&0\\0&0&1\\0&0&0\\0&0&0\\0&0&0\\\end{bmatrix}$\\		
		\end{enumerate}
		
\subsection{5.72}
$\mathrm{rref}\begin{bmatrix}[rrrr]0&1&3&4\\0&3&9&12\\3&1&4&8\\1&2&5&8\\4&1&2&7\\\end{bmatrix} = \begin{bmatrix}[rrrr]1&0&0&1\\0&1&0&1\\0&0&1&1\\0&0&0&0\\0&0&0&0\\\end{bmatrix}$\\
A basis for the column space:\\
This is the transpose of the nonzero rows which is why these form a column space instead of a row space.
$\{\begin{bmatrix}[r]1\\0\\0\\1\\\end{bmatrix},\begin{bmatrix}[r]0\\1\\0\\1\\\end{bmatrix},\begin{bmatrix}[r]0\\0\\1\\1\\\end{bmatrix}\}$

\subsection{5.77}
Use the theorem in the sums and intersections section.
$U=\mathrm{rowspace}\begin{bmatrix}[rrrrr]1&3&-3&-1&-4\\1&4&-1&-2&-2\\2&9&0&-5&-2\\\end{bmatrix} \rightarrow \mathrm{rref}=\begin{bmatrix}[rrrrr]{}\\ {}\\ 0&0&0&0&0\\\end{bmatrix}$\\
Thus $\mathrm{dim}(U)=2$.\\
$W=\mathrm{rowspace}\begin{bmatrix}[rrrrr]1&6&2&-2&3\\2&8&-1&-6&-5\\1&3&-1&-5&-6\\\end{bmatrix} \rightarrow \mathrm{rref}=\begin{bmatrix}[rrrrr]1&0&0&*&*\\0&1&0&*&*\\0&0&1&*&*\\\end{bmatrix}$.\\
Thus $\mathrm{dim}(W)=3$.\\
$\mathrm{dim}(U+W=3)$. To find this, stick the two matrices together into a big 6 x 5 matrix and take the rref of that.\\
$\begin{bmatrix}[r]u-vectors\\ \hline w-vectors\\\end{bmatrix}$\\
$\mathrm{dim}(U\cap W) = \mathrm{dim}(U) + \mathrm{dim}(W) - \mathrm{dim}(U+W) = 2+3-3=2$\\
This system will also be used in 5.75.

\section{Sums and Intersections Continued}
\begin{thrm}
Suppose $V$ is a vector space within subspaces $U$ and $W$. Then 
\[ \mathrm{dim}(u+w)+\mathrm{dim}(U\cap W) = \mathrm{dim}(U)+\mathrm{dim}(W) \]
\end{thrm}

\begin{defn}
$\oplus$ = Direct Sum\\
Setup: $V_1$ and $V_2$ are vector spaces over $\mathbb{F}$(scalar field).\\
\[ V_1 \oplus V_2 = \{(\vec{v_1},\vec{v_2}): \vec{v_1}\in V_1, \vec{v_2}\in V_2\} \]
This is a vector space with the operations:\\
\begin{align*}
(\vec{v_1},\vec{v_2})+(\vec{w_1},\vec{w_2}) :=(\vec{v_1}+\vec{w_1},\vec{v_2}+\vec{w_2})\\
k(\vec{v_1},\vec{v_2}) := (k\vec{v_1},k\vec{v_2})
\end{align*}
\end{defn}
\begin{ex}
$V_1=\mathbb{R}^2 = \{\begin{bmatrix}[r]x\\y\\\end{bmatrix}: x,y\in\mathbb{R}\}$ (as columns)
\[ V_2  =\mathbb{R}^1 = \{[z]: z\in \mathbb{R}\} \]
Basis for $V_1 \oplus V_2$:
\[ \{(\begin{bmatrix}[r]1\\0\\\end{bmatrix},[0]),(\begin{bmatrix}[r]0\\1\\\end{bmatrix},[0]),(\begin{bmatrix}[r]0\\0\\\end{bmatrix},[1])\} \]
Notice $\mathbb{R}^2 \oplus \mathbb{R}^1$ is isomorphic to $\mathbb{R}^3$.
\end{ex}





\mychapter{17}{2018-02-16}
\section{Homework Review}
\subsection{5.79}
Prove $dim(U\oplus W) = dim(U)+dim(W)$.\\
See example 5.42: Prove $dim(U+W)=dim(U)+dim(W)-dim(U\cap W)$.\\
Use the theorem from 5.42.\\
Notice $U\cap W = \{\vec{0}\}$ by hypothesis. This implies two things:\\
\[ dim(U\cap W) = 0 \]
\[ U \oplus W = U + W \]
Thus using the formula, $dim(U \oplus W) = dim(U+W) = dim(U) + dim(W) - dim(U \cap W)$. 

\subsection{4.80.b}
$A=\mathbb{R}^{4 x 8}$\\
$rank(A)\in \{0,1,2,3,4\}$\\
We can get rank 0 if all items in matrix are zero.\\
$colrank(A)\in \{0,1,2,3,4\}$

\section{Direct Sums}
We only use $U \oplus W$ when $U \cap W = \{\vec{0}\}$.

\begin{defn}
Suppose $V$ is a vector space with subspaces $V_1$ and $V_2$. Then $V$ is an internal direct sum of $V_1$ and $V_2$ if both of the following hold:
\begin{enumerate}
\item $V = V_1+V_2$
\item $V_1\cap V_2 = \{\vec{0}\}$
\end{enumerate}
Under these conditions, we can write $V = V_1 \oplus V_2$.
\end{defn}

\mychapter{18}{2018-02-20}
\section{Coordinate Vectors}
\subsection{Change of Basis Matrix}
\begin{defn}
Let $S=[\vec{v_1},\vec{v_2},\vec{v_3},...,\vec{v_n}]$ and $T=[\vec{w_1},\vec{w_2},\vec{w_3},...,\vec{w_n}]$ be bases for a vector space $V$. Then the change of basis matrix is:
\[ Q_T^S = \begin{bmatrix}[rrrrr]C_T(\vec{v_1})&C_T(\vec{v_2})&C_T(\vec{v_3})& \cdots& C_T(\vec{v_n})\\\end{bmatrix} \]
where $C_T(\vec{v})$ denotes the coordinate vector of $\vec{v}$ with respect to $T$. 
\end{defn}
\begin{ex}
$S=[1,t,t^2]$ and $T=[1,1+t,(1+t)^2]$ are bases for $P_2(t)$. Compute the change of basis matrix.\\
$C_T(t^2) = \begin{bmatrix}[r]a_0\\a_1\\a_2\\\end{bmatrix}$ where $t^2 = a_0(1)+a_1(1+t)+a_2(1+2t+t^2) = (a_1+a_1+a_2)(1)+(a_1+2a_2)(t)+a_2(t^2)$
\[ \sysdelim{.}{.}\systeme[a_0a_1a_2]{0=a_0+a_1+a_2,0=a_1+a_2,1=a_2} \]
So $C_T(t^2) = \begin{bmatrix}[r]a_0\\a_1\\a_2\\\end{bmatrix} = \begin{bmatrix}[r]1\\-2\\1\\\end{bmatrix}$
\begin{align*}
Q_T^S &= \begin{bmatrix}[rrr]C_T(1) & C_T(t) & C_T(t^2)\\\end{bmatrix}\\
&=\begin{bmatrix}[rrr]1&-1&1\\0&1&-2\\0&0&1\\\end{bmatrix}
\end{align*} 
\end{ex}
\begin{ex}
Using the same $S$ and $T$ as the previous example, let $\vec{w} = t+3t^2\in \mathrm{span}(S) \in \mathrm{span}(T)$. Compute $C_S(\vec{w})$ and $C_T(\vec{w})$.
$C_S(\vec{w}) = C_S(t+3t^2) = \begin{bmatrix}[r]0\\1\\3\\\end{bmatrix}$.\\
$C_T(\vec{w})=\begin{bmatrix}[r]2\\-5\\3\\\end{bmatrix}$.\\
Notice: $Q_T^SC_S(\vec{w}) = \begin{bmatrix}[rrr]1&-1&1\\0&1&-2\\0&0&1\\\end{bmatrix}\begin{bmatrix}[r]0\\1\\3\\\end{bmatrix} = \begin{bmatrix}[r]2\\-5\\3\\\end{bmatrix}$.\\
In this case, the S's cancel.
\end{ex}
\begin{thrm}
$(Q_T^S)^{-1} = Q_S^T$
\end{thrm}

\section{Linear Mappings}
\begin{defn}
Suppose $V$ and $W$ are vector spaces over a scalar field $\mathbb{F}$. A linear mapping (linear transformation) is a function $V\xrightarrow[]{f} W$ such that the following hold:
\begin{enumerate}
\item $f(\vec{v_1}+\vec{v_2}) = f(\vec{v_1}) + f(\vec{v_2}) $ for all $\vec{v_1},\vec{v_2}\in V$.
\item $f(k\vec{v}) = kf(\vec{v})$ for all $k\in \mathbb{F}, \vec{v}\in V$.
\end{enumerate}
These can also be written as:
\begin{enumerate}
\item f respects or preserves addition.
\item f respects or preserves scalar multiplication.
\end{enumerate}
\end{defn}
The most important example:
\begin{ex}
$v=\mathbb{R}^n$, $w=\mathbb{R}^m$, and $A$ any fixed m x n matrix.\\
Define $f_A(\vec{v}) = A\vec{v}$ (mult by a A from the left). $\vec{v} \in \mathbb{R}^n$.\\
$A(\vec{v_1}+\vec{v_2}) = A\vec{v_1}+A\vec{v_2}$.\\
$A(k\vec{v})=kA\vec{v}$.
\end{ex}
\begin{ex}
$f\begin{bmatrix}[r]x\\y\\z\\\end{bmatrix} = \begin{bmatrix}[r]3x-y+2z\\7x+11y-z\\\end{bmatrix}$. \\
$V = \mathrm{domain} = \mathbb{R}^3$\\
$W= \mathrm{codomain/target} = \mathbb{R}^2$\\
Notice:
\[ \begin{bmatrix}[rrr]3&-1&2\\7&11&-1\\\end{bmatrix}\begin{bmatrix}[r]x\\y\\z\\\end{bmatrix}=f\begin{bmatrix}[r]x\\y\\z\\\end{bmatrix}
 \]
\end{ex}

\mychapter{19}{2018-02-22}
\section{Homework Review}
\subsection{6.34}
Assumption can also be written as $S_2=S_1P$ and $S_3=S_2Q$.
Then $S_3=S_2Q=(S_1P)Q=S_1(PQ)$ since matrix multiplication is associative.

\subsection{8.53}
Part A) We have to check if the codomain of f is the same as the domain of g.\\
Written in form $domain \rightarrow codomain$.
$domain(g)=B$\\
$codomain(F)=B$\\
$domain(g\circ f) = B$\\
$codomain(g \circ f) = B$
Part C) $F\circ f$.\\
$domain(F) = B$\\
$codomain(f) = B$\\
$A\xrightarrow[]{f} B \xrightarrow[]{F} C$\\
$domain(F \circ f) = A$\\
$codomain(F \circ f) = C$\\
Therefore composition is not defined.

\section{Rank Nullity Theorem}
\begin{defn}
Suppose $A$ is a matrix. The nullity of $A$ is the dimension of the nullspace of $A$.
\end{defn}
\begin{ex}
$A=\begin{bmatrix}[rrr]1&2&3\\3&6&9\\\end{bmatrix}$. The nullity of this is $2$.\\
Consider the system $Av=0, v=\begin{bmatrix}[r]x\\y\\z\\\end{bmatrix}$ or $\sysdelim{.}{.}\systeme[xyz]{x+2y+3z=0,3x+6y+9z=0}$.\\
Are there any redundant rows in the matrix (redundant equations)? Yes since row 2 is three times the first row.\\
\[ rref(A)=\begin{bmatrix}[rrr]1&2&3\\0&0&0\\\end{bmatrix} \]
There are two free parameters. $x$ is dependent, $y,z$ are free.\\
\[ \begin{bmatrix}[r]x\\y\\z\\\end{bmatrix} = \begin{bmatrix}[r]-2s-3t\\s\\t\\\end{bmatrix} = s\begin{bmatrix}[r]-2\\1\\0\\\end{bmatrix}+t\begin{bmatrix}[r]-3\\0\\1\\\end{bmatrix} \Rightarrow = \{\begin{bmatrix}[r]-2\\1\\0\\\end{bmatrix},\begin{bmatrix}[r]-3\\0\\1\\\end{bmatrix}\} \]
Then $rank(A)=1$.
\end{ex}
\begin{thrm}
Let $A$ be any m x n matrix. Then $rank(A)+nullity(A)=n$.
\end{thrm}

\section{Image and Kernel}
\begin{defn}
Suppose $V \xrightarrow[]{f} W$ is a linear transformation.\\
(1) The image of $f$ is $im(f)=\{f(\vec{v}): \vec{v}\in V\} \subset W$.\\
The kernel of $f$ is $ker(f)=\{\vec{v}:f(\vec{v})=\vec{0}\}\subset V = f^{-1}(\vec{0})$.
\end{defn}
\begin{ex}
$V=P_2(t), W=\mathbb{R}^2$\\
$f(p)=\begin{bmatrix}[r]P(1)\\P(2)\\\end{bmatrix}$\\
Recall $P_2(t)=\{a_0+a_1t+a_2t^2\} = span\{1,t,t^2\}$\\
$f(t^2-t)=\begin{bmatrix}[r](t^2-t)|_{t=1}\\(t^2-t)|_{t=2}\\\end{bmatrix}=\begin{bmatrix}[r]0\\2\\\end{bmatrix}\Rightarrow \begin{bmatrix}[r]0\\2\\\end{bmatrix}\in im(f)$\\
The diagram for this if $P_2(t)\xrightarrow[]{f} \mathbb{R}^2$.\\
We can also get $\begin{bmatrix}[r]0\\4\\\end{bmatrix} \in im(f)$ with the polynomial $f(2t^2-2t) = \begin{bmatrix}[r]0\\2\\\end{bmatrix}$.\\
Is $\begin{bmatrix}[r]1\\0\\\end{bmatrix} \in im(f)$?\\
Using $f(a_0+a_1t+a_2t^2)$ as a system of linear equations we can solve this. Let
\[ f(a_0+a_1t+a_2t^2) = \begin{bmatrix}[r]a_0+a_1+a_2\\a_0+2a_1+4a_2\\\end{bmatrix} = \begin{bmatrix}[r]1\\0\\\end{bmatrix} \Rightarrow \sysdelim{.}{.}\systeme[a_0a_1a_2]{a_0+a_1+a_2=1,a_0+2a_1+4a_2=0} \]
\end{ex}



\mychapter{20}{2018-02-23}
\section{Homework Review}
\subsection{6.31}
	$S=\{(1,2),(2,3)\}, S^\prime = \{(1,3),(1,4)\}$\\
	Find change of basis matrix from $S$ to $S^\prime$.\\
	$Q_S^{S^\prime} = \begin{bmatrix}[rr]C_S(1,3) & C_S(1,4)\\\end{bmatrix} = \begin{bmatrix}[rr]x_1&x_2\\y_1&y_2\\\end{bmatrix}$ where $x_1(1,2)+y_1(2,3)=(1,3)$ and $x_2(1,2)+y_2(2,3)=(1,4)$.\\
	$x_1+2y_1 = 1$, $2x_1+3y_1=4$, $x_2+2y_2=1$, $2x_2+3y_2=4$.\\
	$\begin{bmatrix}[rr]1&2\\2&3\\\end{bmatrix}\begin{bmatrix}[rr]x_1&x_2\\y_1&y_2\\\end{bmatrix}=\begin{bmatrix}[rr]1&1\\3&4\\\end{bmatrix} \Rightarrow \begin{bmatrix}[rr]x_1&x_2\\y_1&y_2\\\end{bmatrix} = \begin{bmatrix}[rr]1&2\\2&3\\\end{bmatrix}^{-1}\begin{bmatrix}[rr]1&1\\3&4\\\end{bmatrix} = \begin{bmatrix}[rr]3&5\\-1&-2\\\end{bmatrix}$\\
	For part b of this problem just take the inverse of this result.

\section{Image and Kernel Continued}
\subsection{Linear Transformation Example}
\begin{ex}
$V=P_4(t), W=P_4(t)$\\
These are $\mathrm{span}\{1,t,t^2,t^3,t^4\}$ (be prepared to prove this span is linearly independent on the exam)\\
$T(p) = \frac{d}{dt}p=p^\prime$\\
$T(t^3+2t+1)=3t^2+2 \Rightarrow 3t^2+2 \in \mathrm{image}(T)$\\
This all implies that $T$ is a linear transformation from $V$ to $W$.\\
Is $1 \in \mathrm{im}(T)$?\\
$T(t)=1$, $T(\frac{1}{2}t^2) = t$, $T(\frac{1}{3}t^3) = t^2$. This continues up to $t^4$ but $t^4 \not\in \mathrm{im}(T)$.\\
Is $6+7t-14t^2+t^3$ in the image?\\
$T(6t+\frac{7}{2}t^2-\frac{14}{3}t^3+\frac{1}{4}t^4) = 6+7t-14t^2+t^3$.\\
$\mathrm{range}(T)=\{a_0+a_1t+a_2t^2+a_3t^3\} = \mathrm{span}\{1,t,t^2,t^3\}$ (remember that image and range are the same thing)\\
$\mathrm{ker}(T)=\{p: T(p) = 0\} = \mathrm{span}\{1\}$\\
$\mathrm{dim}(\mathrm{ker}(T))=1$\\
$\mathrm{dim}(\mathrm{ker}(T)) + \mathrm{dim}(\mathrm{im}(T)) = \mathrm{dim}(V)$ (dim of kernel + dim of image (or range) is equal to dim of domain)
\end{ex}


\mychapter{21}{2018-02-26}
\section{Exam Problem}
\begin{thrm}
Suppose $V\xrightarrow[]{f} W$ is a linear mapping (transformation). Then (a) The image of $f$ is a vector subspace of $W$, and (b) the kernel of $f$ is a vector subspace of $V$.
\end{thrm}
We will have to prove this theorem on the exam.
\begin{proof}
(Exercise: Use the subspace criterion.)
\end{proof}

\section{Rank Nullity}
\subsection{The Rank Nullity Theorem for Linear Mappings}
\begin{thrm}
Suppose $V \xrightarrow[]{f} W$ is a linear mapping and $V$ is finite dimensional. Then
\[ \mathrm{rank}(f) = \mathrm{nullity}(f) = \mathrm{dim}(V) \]
where $\mathrm{rank}(f) = \mathrm{dim}(\mathrm{image}(V))$ and $\mathrm{nullity}(f)=\mathrm{dim}(\mathrm{ker}(f))$.
\end{thrm}
\subsection{8.75}
\begin{ex}
Given $V=P_{10}(v) = \mathrm{span}\{1,t,t^2,...,t^{10}\}$, $\mathrm{dim}(V)=11$.\\
$f=\frac{d^4}{dt^4}$. Notice: $(P+Q)^{\prime\prime\prime\prime} = P^{\prime\prime\prime\prime} + Q^{\prime\prime\prime\prime}$ and $(kP)^{\prime\prime\prime\prime} = kP^{\prime\prime\prime\prime}$ where $P$ and $Q$ are polynomials and $k$ is a scalar. This shows that $f$ is a linear mapping.\\
We have to find bases and dimensions of (a) $\mathrm{im}(D^4)$ and (b) $\mathrm{ker}(D^4)$.\\
(a) $\mathrm{im}(D^4)=P_6(t) \Rightarrow \mathrm{basis}\{1,t,...,t^6\} \Rightarrow \mathrm{dim} = 7$.\\
(b) $\mathrm{dim}(\mathrm{ker}) = 11-7=4$.\\
$\mathrm{ker} = P_3(t) \Rightarrow \mathrm{basis}=\{1,t,t^2,t^3\}$.\\
The kernel of $D^4$ in this case is equal to the number of polynomials in $V$ where the fourth derivative of those polynomials is equal to zero. $D^4$ represents the fourth derivative.
\end{ex}

\subsection{Alternate Example of Rank Nullity Theorem}
\begin{ex}
Let $V=P_4(t)$. $L(f)=f^{\prime\prime} + tf^\prime$.
\begin{enumerate}
\item[(a)] Prove $L$ is a linear mapping.\\
Notice 
\[ L(f+g) = (f+g)^{\prime\prime} + t(f+g)^\prime = f^{\prime\prime} + g^{\prime\prime} + tf^\prime + tg^\prime = (f^{\prime\prime}+tf^\prime)+(g^{\prime\prime}+tg^\prime) = L(f) + L(g) \] 
Thus $L$ respects addition. Also
\[ l(kf) = (kf)^{\prime\prime}+t(kf)^\prime = kf^{\prime\prime}+ktf^\prime = kL(f) \] 
Thus $L$ respects multiplication.
\item[(b)] Find a basis for the kernel of $L$.\\
$f=a_0+a_1t+a_2t^2+a_3t^3+a_4t^4$. We want $f^{\prime\prime}+tf^\prime = 0$. Second derivative of that function is
\[ (2a_2+6a_3t+12a_4t^3) + t(a_1+2a_2t+3a_3t^2+4a_4t^3) = 2a_2+(6a_3+a_1)t+(12a_4+2a_2)t^2+3a_3t^3+4a_4t^4 \]
We need to solve this by setting it equal to zero. We have a linear system of equations.\\
\begin{align*}
a_0&=\mathrm{free}\\
a_1&=0\\
a_2&=0\\
a_3&=0\\
a_4&=0
\end{align*}
$basis:\{1\}$ or any nonzero constant. Thus $dim(ker)=1$.
\item[(c)] Find a basis for the image of $L$.\\
From the previous result, $dim(im)=4$.
\end{enumerate}
We know $V=\mathrm{span}\{1,t,t^2,t^3,t^4\}$. Now we just plug in our basis into the given formula $L(f)=f^{\prime\prime}+tf^\prime$.
\begin{align*}
L(1)&=0\\
L(t)&=t\\
L(t^2)&=2+2t^2\\
L(t^3)&=6t+3t^3\\
L(t^4)&=12t^2+4t^4
\end{align*}
Since this contains the zero vector, this is not a linearly independent set. The equations above, excluding $L(1)$ yields a basis for $\mathrm{im}(L)$. 
\end{ex}

\section{Matrix Representations of Linear Mappings}
\begin{defn}
Suppose $V$ has the basis $E=\{\vec{v_1},\vec{v_2},...,\vec{v_n}\}$, $W$ has the basis $F=\{\vec{w_1},\vec{w_2},...,\vec{w_n}\}$, and $V\xrightarrow[]{T} W$ is a linear mapping. The matrix of $T$ with respect to the pair $(E,F)$ is
\[ [T]^E_F = \begin{bmatrix}[rrrr]C_V(T(\vec{v_1})) & C_F(T(\vec{v_2})) & \cdots & C_F(T(\vec{v_n}))\\\end{bmatrix} \]
\end{defn}
\begin{ex}
$V=P_3(t)$ with a basis $E=[1,t,t^2,t^3]$\\
$W=P_2(t)$ with a basis $F=[1,t,t^2]$\\
$T(f)=f^\prime = \frac{df}{dt}$\\
Find the matrix of $T$ with respect to $(E,F)$.
\begin{align*}
[T]^E_F &= \begin{bmatrix}[rrrr]C_FT(1) & C_FT(t) & C_FT(t^2) & C_FT(t^3)\\\end{bmatrix}\\
&= \begin{bmatrix}[rrrr]C_F(0) & C_F(1) & C_F(2t) & C_F(3t^2)\\\end{bmatrix}\\
&= \begin{bmatrix}[rrrr]0&1&0&0\\0&0&2&0\\0&0&0&3\\\end{bmatrix}
\end{align*}
\end{ex}

\mychapter{22}{2018-02-27}
\section{Homework Review}
\subsection{8.73}
\[ F\begin{bmatrix}[r]x\\y\\z\\\end{bmatrix} = \begin{bmatrix}[rrr]1&4&0\\2&5&0\\3&6&0\\\end{bmatrix}\begin{bmatrix}[r]x\\y\\z\\\end{bmatrix} = \begin{bmatrix}[r]x+4y\\2x+5y\\3x+6y\\\end{bmatrix} \]
Notice: $F\begin{bmatrix}[r]1\\0\\0\\\end{bmatrix}=\begin{bmatrix}[r]1\\2\\3\\\end{bmatrix} \Rightarrow \begin{bmatrix}[r]1\\2\\3\\\end{bmatrix} \in \mathrm{image}(F)$.\\
For a matrix, image = columnspace.

\subsection{8.85.a}
Does the given system have a nontrivial kernel (is it singular)?
\[ F\begin{bmatrix}[r]x\\y\\z\\\end{bmatrix} = \begin{bmatrix}[rrr]1&1&1\\2&3&5\\1&3&7\\\end{bmatrix}\begin{bmatrix}[r]x\\y\\z\\\end{bmatrix} = \begin{bmatrix}[r]0\\0\\0\\\end{bmatrix} \]
$\mathrm{rank}(F)=2$ so $\mathrm{nullity}(F)=1$.
\[ \mathrm{rref}(F)=\begin{bmatrix}[rrr]1&0&-2\\0&1&3\\0&0&0\\\end{bmatrix} \]
$\mathrm{nullspace}=\mathrm{ker}=\mathrm{span}\{\begin{bmatrix}[r]2\\-3\\1\\\end{bmatrix}\}$. We get this matrix by setting our free variable $z=1$ then solving the system.\\
Not needed to solve this problem but the basis for the range is $\{\begin{bmatrix}[r]1\\2\\1\\\end{bmatrix},\begin{bmatrix}[r]1\\5\\7\\\end{bmatrix}\}$.\\
Notice we can verify our result by multiplying our answer by the original matrix in order to get the zero matrix.
\[ \begin{bmatrix}[rrr]1&1&1\\2&3&5\\1&3&7\\\end{bmatrix}\begin{bmatrix}[r]2\\-3\\1\\\end{bmatrix} = \begin{bmatrix}[r]0\\0\\0\\\end{bmatrix} \]

\subsection{8.80.c}
Assume $F(x,y,z)=(y,x+z)$, $G(x,y,z)=(2z,x-y)$, $H(x,y)=(y,2x)$.\\
\begin{align*}
[H \circ (F+G)](x,y,z) &= H(F(x,y,z))+H(G(x,y,z))\\
&= H(y,x+z)+H(2z,x-y)\\
&= (x+z,2y)+(x-y,4z)\\
&= (2x-y+z,2y+4z)
\end{align*}
In this problem we are just plugging and chugging.\\
We can do this an a different way as well. Start by writing these as matrices.
\[ F\begin{bmatrix}[r]x\\y\\z\\\end{bmatrix}=\begin{bmatrix}[rrr]0&1&0\\1&0&1\\\end{bmatrix}\begin{bmatrix}[r]x\\y\\z\\\end{bmatrix}\]
\[ G\begin{bmatrix}[r]x\\y\\z\\\end{bmatrix}=\begin{bmatrix}[rrr]0&0&2\\1&-1&0\\\end{bmatrix}\begin{bmatrix}[r]x\\y\\z\\\end{bmatrix}\]
\[ H\begin{bmatrix}[r]x\\y\\\end{bmatrix}=\begin{bmatrix}[rr]0&1\\2&0\\\end{bmatrix}\begin{bmatrix}[r]x\\y\\z\\\end{bmatrix}\]
We will refer to these matrices as $M_F, M_G, M_H$.\\
Notice:
\begin{align*}
M_H(M_F+M_G) &= \begin{bmatrix}[rr]0&1\\2&0\\\end{bmatrix}\left(\begin{bmatrix}[rrr]0&1&0\\1&0&1\\\end{bmatrix}+\begin{bmatrix}[rrr]0&0&2\\1&-1&0\\\end{bmatrix}\right )\\
&= \begin{bmatrix}[rr]0&1\\2&0\\\end{bmatrix}\begin{bmatrix}[rrr]0&1&2\\2&-1&1\\\end{bmatrix}\\
&=\begin{bmatrix}[rrr]2&-1&1\\0&2&4\\\end{bmatrix}
\end{align*}
$H(a,b)=(b,2a)M_H$

\subsection{8.70}
\[ F\begin{bmatrix}[r]x\\y\\z\\\end{bmatrix}=\begin{bmatrix}[rrr]1&2&-3\\2&5&-4\\1&4&1\\\end{bmatrix}\begin{bmatrix}[r]x\\y\\z\\\end{bmatrix} \]
\[ \mathrm{rref}=\begin{bmatrix}[rrr]1&0&-7\\0&1&2\\0&0&0\\\end{bmatrix} \]
So $\mathrm{rank}(F)=2=\mathrm{dim}(\mathrm{im}(F))$ and $\left \{\begin{bmatrix}[r]1\\2\\1\\\end{bmatrix},\begin{bmatrix}[r]2\\5\\4\\\end{bmatrix}\right\}$ forms a basis.\\
\[ F\begin{bmatrix}[r]1\\0\\0\\\end{bmatrix}=\begin{bmatrix}[r]1\\2\\1\\\end{bmatrix} \]
Basis for rowspace: $\{[1,2,-3],[2,5,-4]\}$.\\
Crossproduct:
\[ \begin{bmatrix}[rrr]\hat{i} & \hat{j} & \hat{k}\\1&2&-3\\2&5&-4\\\end{bmatrix}=((2)(-4)-(-3)(5)),(-3)(2)-(1)(-4), (1)(5)-(2)(2)) = (7,-2,1) \]
Basis for the nullspace (kernel) is given by $\left\{\begin{bmatrix}[r]7\\-2\\1\\\end{bmatrix}\right\}$.\\
Notice: $\begin{bmatrix}[rrr]1&2&-3\\2&5&-4\\1&4&1\\\end{bmatrix}\begin{bmatrix}[r]7\\-2\\1\\\end{bmatrix}$ will give us the zero matrix.

\section{Singularity}
\begin{defn}
$V \xrightarrow[]{F} U$ is singular if $\mathrm{ker}(F) \neq \{\vec{0}\}$
\end{defn}
This definition is used in problem 8.85 in the previous section.

\mychapter{23}{2018-03-12}
\section{Midterm Review}
\begin{enumerate}
\item[1.] (b) Notice $rank(A)=2$. From there pick two linearly independent columns.
\item[3.] One possible answer for (a):
\[ S=\left \{ \begin{bmatrix}[rrr]0&-1&0\\1&0&0\\0&0&0\\\end{bmatrix},\begin{bmatrix}[rrr]0&0&-1\\0&0&0\\1&0&0\\\end{bmatrix},\begin{bmatrix}[rrr]0&0&0\\0&0&-1\\0&1&0\\\end{bmatrix} \right \} \]
These are all skew symmetric.\\
For part (b), let $H(M)=M+M^T$. The kernel of this is all matrices such that that equation is equal to zero which is in turn the definition of a skew symmetric matrix.
\item[4.] We need $G^{-1}(at^2+bt+c)=(x,y,z)$ where $G(x,y,z)=(x+y)t^2+(2y+2z)t+z = (at^2+bt+c)$. Thus we solve the system for (x,y,z).\\
$a=x+y$, $b=2y+2z$, $c=z$.  
\end{enumerate}

\section{Chapter 7: Geometry}
\subsection{Magnitude and Angle}
\begin{defn}
The magnitude of $(a_1,a_2,...,a_n)\in \mathbb{R}^n$ is $||(a_1,a_2,...,a_n)||=\sqrt{a_1^2+a_2^2+\dots +a_n^2}$.\\
The magnitude of $||\vec{v} = \sqrt{\vec{v}\cdot \vec{v}}$.
\end{defn}
\begin{defn}
Let $\vec{v},\vec{w}\in\mathbb{R}^n$. The angle between $\vec{v}$ and $\vec{w}$ is $\phi_{v,w}$ where
\[ \cos(\phi_{v,w})=\frac{v \cdot w}{||v|| * ||w||} \]
\end{defn}
\begin{ex}
Let $u=(3,0,0)$, $v=(1,1,0)$, $w=(0,2,2)$. Compute (a) The magnitudes; (b) The angles.\\
(a) $||u||=\sqrt{3^2+0^2+0^2} = 3$, $||v|| = \sqrt{2}$, $||w|| = 2\sqrt{2} = \sqrt{8}$.\\
(b) 
\[ \phi_{u,v}=\cos^{-1}(\frac{u\cdot v}{||u||*||v||}) = \cos^{-1}(\frac{3}{\sqrt{2}} = 45^\circ \]
\[ \phi_{u,w}= \cos^{-1}(\frac{u\cdot w}{||u||*||w||}) = \cos^{-1}(\frac{0}{*}) = 90^\circ \]
\[ \phi_{v,w} = \cos^{-1}(\frac{v\cdot w}{||v||*||w||}) = \cos^{-1}(\frac{2}{\sqrt{2}*2\sqrt{2}})=\cos^{-1}(\frac{1}{2})=60^\circ \]
Notice that $u \cdot v = 0$ is the same as saying $u$ and $v$ are perpendicular assuming neither $u$ nor $v$ are equal to zero.
\end{ex}

\subsection{Orthogonality}
\begin{defn}
A subset $S=\{v_1,v_2,...,v_k\}\in \mathbb{R}^n$ is orthogonal if $v_i \cdot v_j = 0$ whenever $i \neq j$.
\end{defn}
\begin{ex}
\[ S=\left \{ \begin{bmatrix}[r]1\\0\\0\\0\\\end{bmatrix},\begin{bmatrix}[r]0\\1\\0\\0\\\end{bmatrix},\begin{bmatrix}[r]0\\0\\1\\0\\\end{bmatrix},\begin{bmatrix}[r]0\\0\\0\\1\\\end{bmatrix}\right\}\in \mathbb{R}^4 \]
is an orthogonal set.
\end{ex}
\subsection{Orthonormal}
\begin{defn}
A subset $S=\{v_1,v_2,...,v_n\}\in \mathbb{R}^n$ is orthonormal if
\[v_i\cdot v_j = \{0 i\neq j,1 i=j\} \]
\end{defn}
\begin{ex}
\[ s=\left\{\begin{bmatrix}[r]1\\0\\0\\\end{bmatrix},\begin{bmatrix}[r]0\\3\\4\\\end{bmatrix},\begin{bmatrix}[r]0\\4\\-3\\\end{bmatrix}\right\}\in\mathbb{R}^3 \]
is orthogonal but it is not orthonormal. The dot product of the first vector with itself is 1 but the dot product of the second set is 25. To get an orthonormal set we divide by the magnitudes.
\[ T+\left\{\begin{bmatrix}[r]1\\0\\0\\\end{bmatrix},\begin{bmatrix}[r]0\\\frac{3}{5}\\\frac{4}{5}\\\end{bmatrix},\begin{bmatrix}[r]0\\\frac{4}{5}\\\frac{-3}{5}\\\end{bmatrix}\right\} \]
is orthonormal.
\end{ex}

\mychapter{24}{2018-03-13}
\section{Homework Review}
\subsection{9.29.b}
$L$ is the reflection in $\mathbb{R}^2$ about the line $y=x$.\\
Think of $L$ as a $2\times 2$ matrix.
\[ L\begin{bmatrix}[r]1\\0\\\end{bmatrix} = \begin{bmatrix}[r]0\\1\\\end{bmatrix} \]
\[ L\begin{bmatrix}[r]0\\1\\\end{bmatrix} = \begin{bmatrix}[r]1\\0\\\end{bmatrix} \]
\[ L\begin{bmatrix}[rr]1&0\\0&1\\\end{bmatrix} = \begin{bmatrix}[rr]0&1\\1&0\\\end{bmatrix} = L \]
\[ L\begin{bmatrix}[r]1\\0\\\end{bmatrix} = \begin{bmatrix}[r]\frac{1}{\sqrt{2}}\\\frac{1}{\sqrt{2}}\\\end{bmatrix} \]

\section{Normalizing Orthogonal Sets}
\begin{defn}
$M\in \mathbb{R}^{m\times n}$ is orthogonal if $MM^T=I_m$.
\[ M=\begin{bmatrix}[r]\vec{r_1}\\\vec{r_2}\\\vdots\\\vec{r_n}\\\end{bmatrix} \]
\[ MM^T = \begin{bmatrix}[r]\vec{r_1}\\\vec{r_2}\\\vdots\\\vec{r_n}\\\end{bmatrix}\begin{bmatrix}[rrrr]\vec{r_1^T} & \vec{r_2^T} & \cdots & \vec{r_n^T}\\\end{bmatrix} \]
\end{defn}
\begin{thrm}
If $M$ is an orthogonal matrix, then $M$ is invertible and $M^{-1}=M^T$.
\end{thrm}

\section{Inner Product Spaces}
\begin{defn}
An inner product space is a pair $(V,f)$ where $V$ is a vector space over $\mathbb{R}$ and $V\times V\xrightarrow[]{f}\mathbb{R}$ is a function such that all of the following hold $\forall u,v,w\in V$ and $\forall \lambda \in \mathbb{R}$:
\begin{enumerate}
\item $f(\vec{v},\vec{u})=f(\vec{u},\vec{v})$ (Symmetry)
\item $f(\lambda \vec{u},\vec{v}) = \lambda f(\vec{u},\vec{v})$ (Linearity)
\item $f(\vec{u}+\vec{w},\vec{v})=f(\vec{u},\vec{v})+f(\vec{w},\vec{v})$ (Linearity)
\item $f(\vec{v},\vec{v}) \geq 0$ (Positive Definiteness)
\item $f(\vec{v},\vec{v})=0$ only when $\vec{v}=0$ (Positive Definiteness)
\end{enumerate}
\end{defn}
\begin{thrm}
$(\mathbb{R}^n, \cdot)$ is an inner product space.
\end{thrm}
\begin{ex}
$V=\mathbb{R}[t]$ is the space of all polynomials in $t$ with coefficients in $\mathbb{R}$.\\
Note: $\{1,t,t^2,t^3,\cdots,t^n\}$ is a basis.\\
Define an inner product by
\[ <p,q>=\int_{-1}^1p(t)q(t)dt \]
Swapping p and q results in the same result so this is symmetric.\\
Since we can take constant out of integrals then it fits linearity.\\
Sum of integral is the same as the integral of sums so it holds addition.\\
Integrating something that is nonnegative gives us an answer that is nonnegative so 4 holds as well.
\end{ex}
\begin{ex}
Find the angle between $p(t)=t$ and $q(t)=t^2$.
\[ \phi_{p,q}=\cos^{-1}(\frac{<p,q>}{||p||*||q||}) \]
\[ ||p|| = \sqrt{<p,q>} = \sqrt{\int_{-1}^1t^2dt}=\sqrt{\frac{2}{3}} \]
\[ ||q|| = \sqrt{\int_{-1}^1t^4dt} = \sqrt{frac{2}{5}}^{-1} \]
\[ <p,q>=\int_{-1}^1t^3dt \]
$\left\{\frac{t}{\sqrt{\frac{2}{3}}},\frac{t^2}{\sqrt{\frac{2}{5}}}\right\}$ is an orthonormal set with respect to the given inner product.
\end{ex}




\mychapter{25}{2018-03-xx}
\end{document}